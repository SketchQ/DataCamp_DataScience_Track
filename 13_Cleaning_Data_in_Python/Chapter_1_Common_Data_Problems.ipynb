{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson I \n",
    "\n",
    "## Data Type Constraints\n",
    "\n",
    "In this course, we're going to understand how to diagnose different problems in our data and how they can come up during our workflow.\n",
    "\n",
    "**In this Course **:\n",
    "\n",
    "* Diagnose dirty data\n",
    "* Side effects of dirty data\n",
    "* Clean Data\n",
    "\n",
    "### Why do we need to clean data?\n",
    "\n",
    "Data Science workflow:\n",
    "\n",
    "* Access Data\n",
    "    - Explore and Process Data\n",
    "        - Extract Insights\n",
    "            - Report Insights\n",
    "\n",
    "Dirty data can appear because of duplicate values, mis-spellings, data-type parsing errors and legacy systems. Without making sure that data is properly cleaned in the exploration\n",
    "and processing phase, we will surely compromise the insight and reports subsequently generated.\n",
    "\n",
    "***Garbage in garbage out.***\n",
    "\n",
    "<img src='pictures/datascienceworklow.jpg' />\n",
    "\n",
    "### Data type Constraints\n",
    "\n",
    "| **DataType** | **Example** | **Python data type** |\n",
    "| -------------|-------------|----------------------|\n",
    "| Text Data | First name, last name, address ... | ``str`` |\n",
    "| Integers | # Subscribers, # products sold ... | ``int`` |\n",
    "| Decimals | Temperature, $ Exchange rates ... | ``float`` |\n",
    "| Binary | Is married, new customer, yes/no ... | ``bool`` |\n",
    "| Dates | Order Dates, ship dates ... | ``datetime``  |\n",
    "| Categories | Marriage status, gender ... | ``category`` |\n",
    "\n",
    "\n",
    "### String to Integers\n",
    "\n",
    "Let's take a look at the following example:\n",
    "\n",
    "```python\n",
    "# Import CSV file and output header\n",
    "sales = pd.read_csv('sales.csv')\n",
    "sales.head(2)\n",
    "```\n",
    "\n",
    "<img src='pictures/saleshead.jpg' />\n",
    "\n",
    "Here's the head of a Data Frame containing revenue generated and quentity of items sold for a sales order. We want to calculate the total revenue generated by all sales orders. As we can see, the Revenue column has the dollar sign on the right hand side.\n",
    "\n",
    "```python\n",
    "# Get data types of columns\n",
    "sales.dtypes    \n",
    "```\n",
    "\n",
    "<img src='pictures/datatypes.jpg' />\n",
    "\n",
    "A close inspection of the DataFrame column's data types using the ``.dtypes`` attribute returns object for the Revenue column.\n",
    "\n",
    "We can also check the data types as well as the number of missing values per column in a DataFrame, by using the ``.info()`` method.\n",
    "\n",
    "```python\n",
    "# Get DataFrame information\n",
    "sales.info()\n",
    "```\n",
    "\n",
    "Since the Revenue column is a string, summing across all sales orders returns one large concatenated string containing each row's string. To fix this, we need to first remove the **$** sign from the string so that pandas is able to convert the strings into numbers without error. \n",
    "\n",
    "\n",
    "```python\n",
    "# Print sum of all Revenue column\n",
    "sales['Revenue'].sum()  # This will return all of the strings concatenated\n",
    "```\n",
    "\n",
    "We do this with the ```.str.strip()``` method, while specifying the string we want to strip as an argument, which is in this case the *dollar sign*. Since our dollar values do not contain decimals, we then convert the Revenue column to an *integer* by using the ```.astype()``` method, specifying the desired data type as argument. \n",
    "\n",
    "```python\n",
    "# Remove $ from Revenue column\n",
    "sales['Revenue'] = sales['Revenue'].str.strip('$')\n",
    "sales['Revenue'] = sales['Revenue'].astype('int')\n",
    "```\n",
    "\n",
    "Had our revenue values been decimal, we would have converted the Revenue column to *float*. We can make sure that the Revenue column is now an integer by using the ```assert``` statement, which takes in a condition as input, as *returns nothing* if that condition is met, and an *error* if it is not.\n",
    "\n",
    "```python\n",
    "# Verify that Revenue is now an integer\n",
    "assert sales['Revenue'].dtype == 'int'\n",
    "```\n",
    "\n",
    "### Numeric or Categorical?\n",
    "\n",
    "A common type of data seems numeric but actually represents categories with a finite set of possible categories. This is called **categorical data**.\n",
    "\n",
    "```python\n",
    "marriage_status\n",
    "3\n",
    "1\n",
    "2\n",
    "```\n",
    "\n",
    "``0`` = Never Married ``1`` = Married ``2`` = Separated ``3`` = Divorced\n",
    "\n",
    "Here we have a marriage status column, which represented by 0, 1, 2, 3. \n",
    "\n",
    "However it will be imported of type integer, which could lead to misleading results when trying to extract some statistical summaries.\n",
    "\n",
    "We can solve this by using ``.astype()`` method seen earlier, but this time specifying the category data type.\n",
    "\n",
    "```python\n",
    "# Convert to categorical\n",
    "df[\"marriage_status\"] = df['marriage_status'].astype('category')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Numeric data or ...?\n",
    "\n",
    "In this exercise, and throughout this chapter, you'll be working with bicycle ride sharing data in San Francisco called ```ride_sharing```. It contains information on the start and end stations, the trip duration, and some user information for a bike sharing service.\n",
    "\n",
    "The ```user_type``` column contains information on whether a user is taking a free ride and takes on the following values:\n",
    "* ``1`` for free riders.\n",
    "* ``2`` for pay per ride.\n",
    "- ``3`` for monthly subscribers.\n",
    "\n",
    "In this instance, you will print the information of ```ride_sharing``` using ```.info()``` and see a firsthand example of how an incorrect data type can flaw your analysis of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25760 entries, 0 to 25759\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       25760 non-null  int64 \n",
      " 1   duration         25760 non-null  object\n",
      " 2   station_A_id     25760 non-null  int64 \n",
      " 3   station_A_name   25760 non-null  object\n",
      " 4   station_B_id     25760 non-null  int64 \n",
      " 5   station_B_name   25760 non-null  object\n",
      " 6   bike_id          25760 non-null  int64 \n",
      " 7   user_type        25760 non-null  int64 \n",
      " 8   user_birth_year  25760 non-null  int64 \n",
      " 9   user_gender      25760 non-null  object\n",
      "dtypes: int64(6), object(4)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "count    25760.000000\n",
      "mean         2.008385\n",
      "std          0.704541\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          3.000000\n",
      "max          3.000000\n",
      "Name: user_type, dtype: float64\n",
      "count     25760\n",
      "unique        3\n",
      "top           2\n",
      "freq      12972\n",
      "Name: user_type_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "\n",
    "# Ride Sharing dataFrame\n",
    "ride_sharing = pd.read_csv('datasets/ride_sharing_new.csv')\n",
    "\n",
    "# Prin the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
    "\n",
    "# Print new summary statistics \n",
    "print(ride_sharing['user_type_cat'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing strings and concatenating numbers\n",
    "\n",
    "In the previous exercise, you were able to identify that ```category``` is the correct data type for ``user_type`` and convert it in order to extract relevant statistical summaries that shed light on the distribution of ```user_type```.\n",
    "\n",
    "Another common data type problem is importing what should be numerical values as strings, as mathematical operations such as summing and multiplication lead to string concatenation, not numerical outputs.\n",
    "\n",
    "In this exercise, you'll be converting the string column ```duration``` to the type ```int```. Before that however, you will need to make sure to strip ```\"minutes\"``` from the column in order to make sure ```pandas``` reads it as numerical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         duration duration_trim  duration_time\n",
      "0      12 minutes           12              12\n",
      "1      24 minutes           24              24\n",
      "2       8 minutes            8               8\n",
      "3       4 minutes            4               4\n",
      "4      11 minutes           11              11\n",
      "...           ...           ...            ...\n",
      "25755  11 minutes           11              11\n",
      "25756  10 minutes           10              10\n",
      "25757  14 minutes           14              14\n",
      "25758  14 minutes           14              14\n",
      "25759  29 minutes           29              29\n",
      "\n",
      "[25760 rows x 3 columns]\n",
      "11.389052795031056\n"
     ]
    }
   ],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing['duration_time'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson II\n",
    "\n",
    "## Data range Constraints\n",
    "\n",
    "Let's first start off with some motivation, Imagine we have a dataset of movies with their respective average rating from a streamin service. The rating can be any integer between 1 and 5.\n",
    "\n",
    "```python\n",
    "movies.head()\n",
    "```\n",
    "\n",
    "<img src='pictures/movies.jpg' />\n",
    "\n",
    "After creating a histogram with matplotlib, we see that there are few movies with an average rating of 6, which is well above the allowable range.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(movies['avg_rating'])\n",
    "plt.title('Average rating of movies (1-5)')\n",
    "```\n",
    "\n",
    "<img src='pictures/movieshist.jpg' />\n",
    "\n",
    "Here's another example, where we see subscription dates in the future for a service. We use the datetime package's ```.date.today()``` function to get today's date, and we filter the dataset by any subscription date higher than today's date. ***We need to pay attention to the range of our data!***\n",
    "\n",
    "```python\n",
    "# Import date time\n",
    "import datetime as dt\n",
    "today_date = dt.date.today()\n",
    "user_signups[user_signups['subscription_date'] > dt.date.today()]\n",
    "```\n",
    "\n",
    "<img src='pictures/subdate.jpg' />\n",
    "\n",
    "### How to deal with out of range Data?\n",
    "\n",
    "* Dropping Data \n",
    "    - Simplest option\n",
    "    - However, we could lose our essential information\n",
    "    - Rule of Thumb:\n",
    "        - Only drop data when a small proportion of your datasets is affected by out of range values.\n",
    "\n",
    "* Setting custom minimums and maximums\n",
    "* Treat as missing and impute\n",
    "* Setting custom value depending on business assumptions\n",
    "\n",
    "#### Movie Example\n",
    "\n",
    "Let's take a look at the movies example. We first isolate the movies with ratings higher than 5. Now if these values are affect a small set of our data, we candrop them.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "# Output Movies with rating > 5\n",
    "movies[movies['avg_rating'] > 5]\n",
    "```\n",
    "\n",
    "**We can drop data in two ways**:\n",
    "\n",
    "* We can either create a new filtered movies DataFrame where we only keep values of ```avg_rating``` lower or equal than to 5.\n",
    "\n",
    "```python\n",
    "# Drop values using filtering\n",
    "movies = movies[movies['avg_rating'] <= 5]\n",
    "```\n",
    "\n",
    "* Or drop the values by using the drop method.\n",
    "\n",
    "```python\n",
    "# Drop values using .drop()\n",
    "movies.drop(movies[movies['avg_rating'] > 5].index, inplace=True)\n",
    "# Assert results\n",
    "assert movies['avg_rating'].max() <= 5 \n",
    "```\n",
    "\n",
    "* We can also change the out of range values to a hard limit.\n",
    "\n",
    "```python\n",
    "# Convert avg_rating > 5 to 5\n",
    "movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5\n",
    "\n",
    "# Asserts statement\n",
    "assert movies['avg_rating'].max() <= 5\n",
    "```\n",
    "\n",
    "#### Date Range example\n",
    "\n",
    "We first look at the data types of the column with the ```.dtypes``` attribute. We can confirm that the ```subscription_date``` column is an *object* and not a *date* or *datetime* object. To compare a pandas object to a date, the first step is to *convert* it to another date. \n",
    "\n",
    "```python\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "# Output data types\n",
    "user_signups.dtypes\n",
    "```\n",
    "\n",
    "We do so by first converting it into a pandas ```datetime``` object with the ```to_datetime()``` function from pandas, which takes in as an argument the column we want to convert. \n",
    "\n",
    "We then need to convert the ```datetime``` object into a ```date```. This conversion is done by appending ```dt.date``` to the code. \n",
    "\n",
    "```python\n",
    "# Conver to date\n",
    "user_signups['subscription_date'] = pd.to_datetime(user_signups['subscription_date']).dt.date\n",
    "```\n",
    "\n",
    "Could we have converted from an object directly to a date, without the pandas datetime conversion in the middle? ***Yes!*** But we'd have had to provide information about the date's format as a string, so it's just as easy to do it this way.\n",
    "\n",
    "\n",
    "Now that the column is a ```date```, we can treat it in a variety of ways. We first create a ```today_date``` variable using the datetime function ```date.today()```, which allows us to store today's date.\n",
    "\n",
    "```python\n",
    "today_date = dt.date.today()\n",
    "```\n",
    "\n",
    "**Drop the Data**\n",
    "\n",
    "```python\n",
    "# Drop values using filtering\n",
    "user_signups = user_signups[user_signups['subscription_date'] < today_date]\n",
    "# Drop values using .drop()\n",
    "user_signups.drop(user_signups[user_signups['subscription_date'] > today_date].index, inplace = True)\n",
    "```\n",
    "\n",
    "**Hardcode dates with upper limit**\n",
    "\n",
    "```python\n",
    "# Drop values using filtering\n",
    "user_signups.loc[user_signups['subscription_date'] > today_date, 'subscription_date'] = today_date\n",
    "# Assert is true\n",
    "assert user_signups.subscription_date.max().date() <= today_date\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Tire Size Constraints\n",
    "\n",
    "In this lesson, you're going to build on top of the work you've been doing with the ```ride_sharing``` DataFrame. You'll be working with the ```tire_sizes``` column which contains data on each bike's tire size.\n",
    "\n",
    "Bicycle tire sizes could be either *26″, 27″ or 29″* and are here correctly stored as a categorical value. In an effort to cut maintenance costs, the ride sharing provider decided to set the maximum tire size to be *27″*.\n",
    "\n",
    "In this exercise, you will make sure the ```tire_sizes``` column has the correct range by first converting it to an *integer*, then setting and testing the new upper limit of 27″ for tire sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the Future\n",
    "\n",
    "A new update to the data pipeline feeding into the ```ride_sharing``` DataFrame has been updated to register each ride's date. This information is stored in the ```ride_date``` column of the type object, which represents strings in pandas.\n",
    "\n",
    "A bug was discovered which was relaying rides taken today as taken next year. To fix this, you will find all instances of the ```ride_date``` column that occur anytime in the *future*, and set the maximum possible value of this column to ```today's date```. Before doing so, you would need to convert ```ride_date``` to a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datetime\n",
    "import datetime as dt \n",
    "\n",
    "# Convert ride_date to date\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_dt']).dt.date\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson III\n",
    "\n",
    "## Uniqueness Constraints\n",
    "\n",
    "Another common data cleaning problemi **duplicate values**\n",
    "\n",
    "Duplicate values can be diagnosed when we have the same exact information repeated across multiple rows, for a some or all columns in our DataFrame.\n",
    "\n",
    "**All Columns have the Same Values**\n",
    "\n",
    "| **First Name** | **Last Name** | **Adress** | **height** | **weight** |\n",
    "| -----------|-----------|--------|--------|--------|\n",
    "| Justin | Saddlemyer | Boulevard du Jardin Botanique 3, Bruxelles | 193 cm | 87 kg |\n",
    "| Justin | Saddlemyer | Boulevard du Jardin Botanique 3, Bruxelles | 193 cm | 87 kg |\n",
    "\n",
    "**Most Columns have the same values**\n",
    "\n",
    "| **First Name** | **Last Name** | **Adress** | **height** | **weight** |\n",
    "| -----------|-----------|--------|--------|--------|\n",
    "| Justin | Saddlemyer | Boulevard du Jardin Botanique 3, Bruxelles | 193 cm | 87 kg |\n",
    "| Justin | Saddlemyer | Boulevard du Jardin Botanique 3, Bruxelles | **194 cm** | 87 kg |\n",
    "\n",
    "### Why do They Happen ?\n",
    "\n",
    "* Data entry and human errors\n",
    "* Bugs and design errors\n",
    "* Join or merge Errors\n",
    "\n",
    "### How to find duplicate values?\n",
    "\n",
    "In this example, we're working with a bigger version of the height and weight data seen earlier.\n",
    "\n",
    "```python\n",
    "# Print the header\n",
    "height_weight.head()\n",
    "```\n",
    "\n",
    "<img src='pictures/duplicate.jpg' />\n",
    "\n",
    "We can find duplicated in a DataFrame by using the ```.duplicated()``` method. It returns a Series of boolean values that are True for duplicate values, and False for non-duplicated values.\n",
    "\n",
    "```python\n",
    "# Get duplicates across all columns\n",
    "duplicates = height_weight.duplicated()\n",
    "height_weight[duplicates]\n",
    "```\n",
    "\n",
    "<img src='pictures/duplicate1.jpg' />\n",
    "\n",
    "We can see exactly which rows are affected by using brackets as such. However, using ```.duplicated()``` without playing around with the arguments of the method can lead to misleading results, as all the columns are required to have duplicate values by default, with all duplicate values being marked as ``True`` except for the first occurrence. \n",
    "\n",
    "This limits our ability to properly diagnose what type of duplication we have, and how to effectively treat it.\n",
    "\n",
    "To properly calibrate how we go about finding duplicates, we will use 2 arguments from the ``.duplicated()`` method:\n",
    "\n",
    "* ``subset`` : List of column names to check for duplication\n",
    "    - For Example; it allows us to find duplicates for the first and last name columns only\n",
    "* ``keep`` : Whether to keep **first**(``'first'``), **last**(``'last'``) or **all**(``False``) duplicate values.\n",
    "\n",
    "In this example, we're checking for duplicates across the first name, last name, and address variables, and we're choosing to keep all duplicates.\n",
    "\n",
    "```python\n",
    "# Column names to check for duplication\n",
    "column_names = ['first_name','last_name', 'address']\n",
    "duplicates = height_weight.duplicated(subset= column_names, keep=False)\n",
    "```\n",
    "\n",
    "To get a better bird's eye view of the duplicates, we sort the duplicate rows using the ```.sort_values()``` method, choosing ```first_name``` to sort by:\n",
    "\n",
    "```python\n",
    "# Output duplicate values\n",
    "height_weight[duplicates].sort_values(by= 'first_name')\n",
    "```\n",
    "\n",
    "### How to treat duplicate values?\n",
    "\n",
    "The complete duplicates can be treated easily. All that required is to keep one of them only and discard the others. This can be done with the ``.drop_duplicates()`` method, as well as the inplace argument which drops the duplicated values directly inside the ``height_weight`` DataFrame.\n",
    "\n",
    "Here we are dropping complete duplicates only, so it's not necessary nor advisable to set a subset, and since the keep argument takes in first as default, we can keep it as such. Note that we can also set it as last, but not as False as it would keep all duplicates.\n",
    "\n",
    "```python\n",
    "# Drop duplicates\n",
    "height_weight.drop_duplicates(inplace=True)\n",
    "```\n",
    "\n",
    "This leaves us with the other 2 sets of duplicates discussed earlier, which are the same for first_name, last_name and address, but contain discrepancies in height and weight. Apart from dropping rows with really small discrepancies, we can use a statistical measure to combine each set of duplicated values.\n",
    "\n",
    "```python\n",
    "# Output duplicate values\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "duplicates = height_weight.duplicated(subset=column_names, keep=False)\n",
    "height_weight[duplicates].sort_values(by='first_name')\n",
    "```\n",
    "For example, we can combine these two rows into one by computing the average mean between them, or the maximum, or other statistical measures, this is highly dependent on a common sense understanding of our data, and what type of data we have.\n",
    "\n",
    "<img src='pictures/duplicates2.jpg' />\n",
    "\n",
    "We can do this easily using the groupby method, which when chained with the agg method, lets you group by a set of common columns and return statistical values for specific columns when the aggregation is being performed.\n",
    "\n",
    "**The ``.groupby()`` and ``.agg()`` methods**\n",
    "\n",
    "```python\n",
    "# Group by column names and produce statistical summaries\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "summaries = {'height':'max', 'weight':'mean'}\n",
    "height_weight = height_weight.groupby(by= column_names).agg(summaries).reset_index()\n",
    "# Make sure aggregation is done\n",
    "duplicates = height_weight.duplicated(subset= column_name, keep= False)\n",
    "height_weight[duplicates].sort_values(by= 'first_name')\n",
    "```\n",
    "\n",
    "For example here, we created a dictionary called ``summaries``, which instructs *groupby* to return the maximum of duplicated rows for the height column, and the mean duplicated rows for the weight column. \n",
    "\n",
    "We then group ``height_weight`` by the *column names* defined earlier, and chained it with the ``agg`` method, which takes in the *summaries* dictionary we created. We chain this entire line with the ``.reset_index()`` method, so that we can have numbered indices in the final output. \n",
    "\n",
    "We can verify that there are no more duplicate values by running the duplicated method again, and use brackets to output duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Finding duplicates\n",
    "\n",
    "A new update to the data pipeline feeding into ```ride_sharing``` has added the ``ride_id`` column, which represents a unique identifier for each ride.\n",
    "\n",
    "The update however coincided with radically shorter average ride duration times and irregular user birth dates set in the future. Most importantly, the number of rides taken has increased by *20%* overnight, leading you to think there might be both complete and incomplete duplicates in the ``ride_sharing`` DataFrame.\n",
    "\n",
    "In this exercise, you will confirm this suspicion by finding those duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       bike_id    duration  user_birth_year\n",
      "3638        11  12 minutes             1988\n",
      "6088        11   5 minutes             1985\n",
      "10857       11   4 minutes             1987\n",
      "10045       27  13 minutes             1989\n",
      "16104       27  10 minutes             1970\n",
      "...        ...         ...              ...\n",
      "8812      6638  10 minutes             1986\n",
      "6815      6638   5 minutes             1995\n",
      "8456      6638   7 minutes             1983\n",
      "8300      6638   6 minutes             1962\n",
      "8380      6638   8 minutes             1984\n",
      "\n",
      "[25717 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(subset='bike_id', keep= False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('bike_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['bike_id','duration','user_birth_year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating duplicates\n",
    "\n",
    "In the last exercise, you were able to verify that the new update feeding into ``ride_sharing`` contains a bug generating both complete and incomplete duplicated rows for some values of the ``ride_id`` column, with occasional discrepant values for the ``user_birth_year`` and ``duration`` columns.\n",
    "\n",
    "In this exercise, you will be treating those duplicated rows by first dropping complete duplicates, and then merging the incomplete duplicate rows into one while keeping the average duration, and the minimum ``user_birth_year`` for each set of incomplete duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not convert 12 minutes5 minutes4 minutes to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1578\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1577\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1578\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49m_cython_operation(\n\u001b[0;32m   1579\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39maggregate\u001b[39;49m\u001b[39m\"\u001b[39;49m, values, how, axis\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mndim \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, min_count\u001b[39m=\u001b[39;49mmin_count\n\u001b[0;32m   1580\u001b[0m     )\n\u001b[0;32m   1581\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m   1582\u001b[0m     \u001b[39m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[0;32m   1583\u001b[0m     \u001b[39m# and non-applicable functions\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m     \u001b[39m# try to python agg\u001b[39;00m\n\u001b[0;32m   1585\u001b[0m     \u001b[39m# TODO: shouldn't min_count matter?\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:939\u001b[0m, in \u001b[0;36mBaseGrouper._cython_operation\u001b[1;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m ngroups \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngroups\n\u001b[1;32m--> 939\u001b[0m \u001b[39mreturn\u001b[39;00m cy_op\u001b[39m.\u001b[39;49mcython_operation(\n\u001b[0;32m    940\u001b[0m     values\u001b[39m=\u001b[39;49mvalues,\n\u001b[0;32m    941\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    942\u001b[0m     min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m    943\u001b[0m     comp_ids\u001b[39m=\u001b[39;49mids,\n\u001b[0;32m    944\u001b[0m     ngroups\u001b[39m=\u001b[39;49mngroups,\n\u001b[0;32m    945\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    946\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:626\u001b[0m, in \u001b[0;36mWrappedCythonOp.cython_operation\u001b[1;34m(self, values, axis, min_count, comp_ids, ngroups, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ea_wrap_cython_operation(\n\u001b[0;32m    619\u001b[0m         values,\n\u001b[0;32m    620\u001b[0m         min_count\u001b[39m=\u001b[39mmin_count,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cython_op_ndim_compat(\n\u001b[0;32m    627\u001b[0m     values,\n\u001b[0;32m    628\u001b[0m     min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m    629\u001b[0m     ngroups\u001b[39m=\u001b[39;49mngroups,\n\u001b[0;32m    630\u001b[0m     comp_ids\u001b[39m=\u001b[39;49mcomp_ids,\n\u001b[0;32m    631\u001b[0m     mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    632\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    633\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:451\u001b[0m, in \u001b[0;36mWrappedCythonOp._cython_op_ndim_compat\u001b[1;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m     result_mask \u001b[39m=\u001b[39m result_mask[\u001b[39mNone\u001b[39;00m, :]\n\u001b[1;32m--> 451\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_cython_op(\n\u001b[0;32m    452\u001b[0m     values2d,\n\u001b[0;32m    453\u001b[0m     min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m    454\u001b[0m     ngroups\u001b[39m=\u001b[39;49mngroups,\n\u001b[0;32m    455\u001b[0m     comp_ids\u001b[39m=\u001b[39;49mcomp_ids,\n\u001b[0;32m    456\u001b[0m     mask\u001b[39m=\u001b[39;49mmask,\n\u001b[0;32m    457\u001b[0m     result_mask\u001b[39m=\u001b[39;49mresult_mask,\n\u001b[0;32m    458\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    459\u001b[0m )\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m res\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:516\u001b[0m, in \u001b[0;36mWrappedCythonOp._call_cython_op\u001b[1;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m out_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_output_shape(ngroups, values)\n\u001b[1;32m--> 516\u001b[0m func, values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_cython_func_and_vals(values, is_numeric)\n\u001b[0;32m    517\u001b[0m out_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_out_dtype(values\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:199\u001b[0m, in \u001b[0;36mWrappedCythonOp.get_cython_func_and_vals\u001b[1;34m(self, values, is_numeric)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m func, values\n\u001b[1;32m--> 199\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_cython_function(kind, how, values\u001b[39m.\u001b[39;49mdtype, is_numeric)\n\u001b[0;32m    201\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:164\u001b[0m, in \u001b[0;36mWrappedCythonOp._get_cython_function\u001b[1;34m(cls, kind, how, dtype, is_numeric)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39m__signatures__:\n\u001b[0;32m    163\u001b[0m     \u001b[39m# raise NotImplementedError here rather than TypeError later\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    165\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfunction is not implemented for this dtype: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[how->\u001b[39m\u001b[39m{\u001b[39;00mhow\u001b[39m}\u001b[39;00m\u001b[39m,dtype->\u001b[39m\u001b[39m{\u001b[39;00mdtype_str\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m f\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: function is not implemented for this dtype: [how->mean,dtype->object]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:1622\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1621\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1622\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39;49m(x)\n\u001b[0;32m   1623\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1624\u001b[0m     \u001b[39m# e.g. \"1+1j\" or \"foo\"\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '12 minutes5 minutes4 minutes'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:1626\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1625\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1626\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mcomplex\u001b[39;49m(x)\n\u001b[0;32m   1627\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1628\u001b[0m     \u001b[39m# e.g. \"foo\"\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: complex() arg is a malformed string",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\devil\\Desktop\\Software Eğitimler\\DataCamp - Python\\Courses\\13_Cleaning_Data_in_Python\\Chapter_1_Common_Data_Problems.ipynb Hücre 15\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/devil/Desktop/Software%20E%C4%9Fitimler/DataCamp%20-%20Python/Courses/13_Cleaning_Data_in_Python/Chapter_1_Common_Data_Problems.ipynb#ch0000014?line=4'>5</a>\u001b[0m statistics \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39muser_birth_year\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/devil/Desktop/Software%20E%C4%9Fitimler/DataCamp%20-%20Python/Courses/13_Cleaning_Data_in_Python/Chapter_1_Common_Data_Problems.ipynb#ch0000014?line=6'>7</a>\u001b[0m \u001b[39m# Group by ride_id and compute new statistics\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/devil/Desktop/Software%20E%C4%9Fitimler/DataCamp%20-%20Python/Courses/13_Cleaning_Data_in_Python/Chapter_1_Common_Data_Problems.ipynb#ch0000014?line=7'>8</a>\u001b[0m ride_unique \u001b[39m=\u001b[39m ride_dup\u001b[39m.\u001b[39;49mgroupby(\u001b[39m'\u001b[39;49m\u001b[39mbike_id\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49magg(statistics)\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/devil/Desktop/Software%20E%C4%9Fitimler/DataCamp%20-%20Python/Courses/13_Cleaning_Data_in_Python/Chapter_1_Common_Data_Problems.ipynb#ch0000014?line=9'>10</a>\u001b[0m \u001b[39m# Find duplicated values again\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/devil/Desktop/Software%20E%C4%9Fitimler/DataCamp%20-%20Python/Courses/13_Cleaning_Data_in_Python/Chapter_1_Common_Data_Problems.ipynb#ch0000014?line=10'>11</a>\u001b[0m duplicates \u001b[39m=\u001b[39m ride_unique\u001b[39m.\u001b[39mduplicated(subset \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbike_id\u001b[39m\u001b[39m'\u001b[39m, keep \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:869\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    866\u001b[0m func \u001b[39m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[0;32m    868\u001b[0m op \u001b[39m=\u001b[39m GroupByApply(\u001b[39mself\u001b[39m, func, args, kwargs)\n\u001b[1;32m--> 869\u001b[0m result \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39;49magg()\n\u001b[0;32m    870\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dict_like(func) \u001b[39mand\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:168\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m is_dict_like(arg):\n\u001b[1;32m--> 168\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magg_dict_like()\n\u001b[0;32m    169\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(arg):\n\u001b[0;32m    170\u001b[0m     \u001b[39m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magg_list_like()\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:475\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    472\u001b[0m     results \u001b[39m=\u001b[39m {key: colg\u001b[39m.\u001b[39magg(how) \u001b[39mfor\u001b[39;00m key, how \u001b[39min\u001b[39;00m arg\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m    473\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m     \u001b[39m# key used for column selection and output\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     results \u001b[39m=\u001b[39m {\n\u001b[0;32m    476\u001b[0m         key: obj\u001b[39m.\u001b[39m_gotitem(key, ndim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39magg(how) \u001b[39mfor\u001b[39;00m key, how \u001b[39min\u001b[39;00m arg\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    477\u001b[0m     }\n\u001b[0;32m    479\u001b[0m \u001b[39m# set the final keys\u001b[39;00m\n\u001b[0;32m    480\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(arg\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:476\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    472\u001b[0m     results \u001b[39m=\u001b[39m {key: colg\u001b[39m.\u001b[39magg(how) \u001b[39mfor\u001b[39;00m key, how \u001b[39min\u001b[39;00m arg\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m    473\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m     \u001b[39m# key used for column selection and output\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     results \u001b[39m=\u001b[39m {\n\u001b[1;32m--> 476\u001b[0m         key: obj\u001b[39m.\u001b[39;49m_gotitem(key, ndim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49magg(how) \u001b[39mfor\u001b[39;00m key, how \u001b[39min\u001b[39;00m arg\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    477\u001b[0m     }\n\u001b[0;32m    479\u001b[0m \u001b[39m# set the final keys\u001b[39;00m\n\u001b[0;32m    480\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(arg\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:265\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, func)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    267\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, abc\u001b[39m.\u001b[39mIterable):\n\u001b[0;32m    268\u001b[0m     \u001b[39m# Catch instances of lists / tuples\u001b[39;00m\n\u001b[0;32m    269\u001b[0m     \u001b[39m# but not the class list / tuple itself.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m     func \u001b[39m=\u001b[39m maybe_mangle_lambdas(func)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1956\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[1;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[0;32m   1954\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numba_agg_general(sliding_mean, engine_kwargs, \u001b[39m\"\u001b[39m\u001b[39mgroupby_mean\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1955\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1956\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cython_agg_general(\n\u001b[0;32m   1957\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1958\u001b[0m         alt\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: Series(x)\u001b[39m.\u001b[39;49mmean(numeric_only\u001b[39m=\u001b[39;49mnumeric_only_bool),\n\u001b[0;32m   1959\u001b[0m         numeric_only\u001b[39m=\u001b[39;49mnumeric_only_bool,\n\u001b[0;32m   1960\u001b[0m     )\n\u001b[0;32m   1961\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1592\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1588\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m   1590\u001b[0m \u001b[39m# TypeError -> we may have an exception in trying to aggregate\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m \u001b[39m#  continue and exclude the block\u001b[39;00m\n\u001b[1;32m-> 1592\u001b[0m new_mgr \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mgrouped_reduce(array_func, ignore_failures\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1594\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_ser \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(new_mgr) \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[0;32m   1595\u001b[0m     warn_dropping_nuisance_columns_deprecated(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m), how)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\base.py:199\u001b[0m, in \u001b[0;36mSingleDataManager.grouped_reduce\u001b[1;34m(self, func, ignore_failures)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39mignore_failures : bool, default False\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m    Not used; for compatibility with ArrayManager/BlockManager.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray\n\u001b[1;32m--> 199\u001b[0m res \u001b[39m=\u001b[39m func(arr)\n\u001b[0;32m    200\u001b[0m index \u001b[39m=\u001b[39m default_index(\u001b[39mlen\u001b[39m(res))\n\u001b[0;32m    202\u001b[0m mgr \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfrom_array(res, index)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1586\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1578\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouper\u001b[39m.\u001b[39m_cython_operation(\n\u001b[0;32m   1579\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m\"\u001b[39m, values, how, axis\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mndim \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, min_count\u001b[39m=\u001b[39mmin_count\n\u001b[0;32m   1580\u001b[0m     )\n\u001b[0;32m   1581\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m   1582\u001b[0m     \u001b[39m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[0;32m   1583\u001b[0m     \u001b[39m# and non-applicable functions\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m     \u001b[39m# try to python agg\u001b[39;00m\n\u001b[0;32m   1585\u001b[0m     \u001b[39m# TODO: shouldn't min_count matter?\u001b[39;00m\n\u001b[1;32m-> 1586\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_agg_py_fallback(values, ndim\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mndim, alt\u001b[39m=\u001b[39;49malt)\n\u001b[0;32m   1588\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1540\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[1;34m(self, values, ndim, alt)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     ser \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\n\u001b[0;32m   1537\u001b[0m \u001b[39m# We do not get here with UDFs, so we know that our dtype\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39m#  should always be preserved by the implemented aggregations\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m \u001b[39m# TODO: Is this exactly right; see WrappedCythonOp get_result_dtype?\u001b[39;00m\n\u001b[1;32m-> 1540\u001b[0m res_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49magg_series(ser, alt, preserve_dtype\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1542\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(values, Categorical):\n\u001b[0;32m   1543\u001b[0m     \u001b[39m# Because we only get here with known dtype-preserving\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m     \u001b[39m#  reductions, we cast back to Categorical.\u001b[39;00m\n\u001b[0;32m   1545\u001b[0m     \u001b[39m# TODO: if we ever get \"rank\" working, exclude it here.\u001b[39;00m\n\u001b[0;32m   1546\u001b[0m     res_values \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(values)\u001b[39m.\u001b[39m_from_sequence(res_values, dtype\u001b[39m=\u001b[39mvalues\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:981\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[1;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[0;32m    978\u001b[0m     preserve_dtype \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 981\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_aggregate_series_pure_python(obj, func)\n\u001b[0;32m    983\u001b[0m npvalues \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    984\u001b[0m \u001b[39mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:1005\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[39mfor\u001b[39;00m i, group \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(splitter):\n\u001b[0;32m   1004\u001b[0m     group \u001b[39m=\u001b[39m group\u001b[39m.\u001b[39m__finalize__(obj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1005\u001b[0m     res \u001b[39m=\u001b[39m func(group)\n\u001b[0;32m   1006\u001b[0m     res \u001b[39m=\u001b[39m libreduction\u001b[39m.\u001b[39mextract_result(res)\n\u001b[0;32m   1008\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m initialized:\n\u001b[0;32m   1009\u001b[0m         \u001b[39m# We only do this validation on the first iteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1958\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1954\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numba_agg_general(sliding_mean, engine_kwargs, \u001b[39m\"\u001b[39m\u001b[39mgroupby_mean\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1955\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1956\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cython_agg_general(\n\u001b[0;32m   1957\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m-> 1958\u001b[0m         alt\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: Series(x)\u001b[39m.\u001b[39;49mmean(numeric_only\u001b[39m=\u001b[39;49mnumeric_only_bool),\n\u001b[0;32m   1959\u001b[0m         numeric_only\u001b[39m=\u001b[39mnumeric_only_bool,\n\u001b[0;32m   1960\u001b[0m     )\n\u001b[0;32m   1961\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:11117\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.mean\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11099\u001b[0m \u001b[39m@doc\u001b[39m(\n\u001b[0;32m  11100\u001b[0m     _num_doc,\n\u001b[0;32m  11101\u001b[0m     desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReturn the mean of the values over the requested axis.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11115\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m  11116\u001b[0m ):\n\u001b[1;32m> 11117\u001b[0m     \u001b[39mreturn\u001b[39;00m NDFrame\u001b[39m.\u001b[39;49mmean(\u001b[39mself\u001b[39;49m, axis, skipna, level, numeric_only, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:10687\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean\u001b[39m(\n\u001b[0;32m  10680\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m  10681\u001b[0m     axis: Axis \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m lib\u001b[39m.\u001b[39mNoDefault \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mno_default,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10685\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m  10686\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series \u001b[39m|\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m> 10687\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stat_function(\n\u001b[0;32m  10688\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m, nanops\u001b[39m.\u001b[39;49mnanmean, axis, skipna, level, numeric_only, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m  10689\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:10639\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10629\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m  10630\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing the level keyword in DataFrame and Series aggregations is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m  10631\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdeprecated and will be removed in a future version. Use groupby \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10634\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m  10635\u001b[0m     )\n\u001b[0;32m  10636\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agg_by_level(\n\u001b[0;32m  10637\u001b[0m         name, axis\u001b[39m=\u001b[39maxis, level\u001b[39m=\u001b[39mlevel, skipna\u001b[39m=\u001b[39mskipna, numeric_only\u001b[39m=\u001b[39mnumeric_only\n\u001b[0;32m  10638\u001b[0m     )\n\u001b[1;32m> 10639\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reduce(\n\u001b[0;32m  10640\u001b[0m     func, name\u001b[39m=\u001b[39;49mname, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, numeric_only\u001b[39m=\u001b[39;49mnumeric_only\n\u001b[0;32m  10641\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4471\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   4467\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   4468\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSeries.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m does not implement \u001b[39m\u001b[39m{\u001b[39;00mkwd_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4469\u001b[0m     )\n\u001b[0;32m   4470\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 4471\u001b[0m     \u001b[39mreturn\u001b[39;00m op(delegate, skipna\u001b[39m=\u001b[39;49mskipna, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:93\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(invalid\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     94\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     95\u001b[0m     \u001b[39m# we want to transform an object array\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[39m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[39m# object arrays that contain strings\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m is_object_dtype(args[\u001b[39m0\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:155\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    153\u001b[0m         result \u001b[39m=\u001b[39m alt(values, axis\u001b[39m=\u001b[39maxis, skipna\u001b[39m=\u001b[39mskipna, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     result \u001b[39m=\u001b[39m alt(values, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:410\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[1;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m datetimelike \u001b[39mand\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     mask \u001b[39m=\u001b[39m isna(values)\n\u001b[1;32m--> 410\u001b[0m result \u001b[39m=\u001b[39m func(values, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, mask\u001b[39m=\u001b[39;49mmask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    412\u001b[0m \u001b[39mif\u001b[39;00m datetimelike:\n\u001b[0;32m    413\u001b[0m     result \u001b[39m=\u001b[39m _wrap_results(result, orig_values\u001b[39m.\u001b[39mdtype, fill_value\u001b[39m=\u001b[39miNaT)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:698\u001b[0m, in \u001b[0;36mnanmean\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    695\u001b[0m     dtype_count \u001b[39m=\u001b[39m dtype\n\u001b[0;32m    697\u001b[0m count \u001b[39m=\u001b[39m _get_counts(values\u001b[39m.\u001b[39mshape, mask, axis, dtype\u001b[39m=\u001b[39mdtype_count)\n\u001b[1;32m--> 698\u001b[0m the_sum \u001b[39m=\u001b[39m _ensure_numeric(values\u001b[39m.\u001b[39;49msum(axis, dtype\u001b[39m=\u001b[39;49mdtype_sum))\n\u001b[0;32m    700\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(the_sum, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    701\u001b[0m     count \u001b[39m=\u001b[39m cast(np\u001b[39m.\u001b[39mndarray, count)\n",
      "File \u001b[1;32mc:\\Users\\devil\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:1629\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1626\u001b[0m             x \u001b[39m=\u001b[39m \u001b[39mcomplex\u001b[39m(x)\n\u001b[0;32m   1627\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1628\u001b[0m             \u001b[39m# e.g. \"foo\"\u001b[39;00m\n\u001b[1;32m-> 1629\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not convert \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m to numeric\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   1630\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not convert 12 minutes5 minutes4 minutes to numeric"
     ]
    }
   ],
   "source": [
    "# CODE WILL ONLY WORK ON DATACAMP WORKSPACE\n",
    "\n",
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby('bike_id').agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset = 'bike_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c59e6e5379e7fd956ce48a476e9664867cfb6229c9530fa6fd87fb84f21040f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
