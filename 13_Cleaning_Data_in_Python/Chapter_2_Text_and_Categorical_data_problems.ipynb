{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson I\n",
    "\n",
    "## Membership Constraints\n",
    "\n",
    "In this chapter, we're going to take a look at common data problems with text and categorical data.\n",
    "\n",
    "In this lesson, we'll focus on categorical variables. **Categorical data** represents variables that represent predefined finite set of categories.\n",
    "\n",
    "| **Type of Data** | **Example Values** | **Numeric Representation** |\n",
    "| -------------|----------------|------------------------|\n",
    "| Marriage status | ``unmarried``, ``married`` | ``0``, ``1`` |\n",
    "| Household income Category | ``0-20K``, ``20-40K``, ... | ``0``, ``1``, ... |\n",
    "| Loan Status | ``default``, ``payed``, ``no_loan`` | ``0``, ``1``, ``2`` |\n",
    "\n",
    "To run machine learning models on categorical data, they are often coded as numbers. Since categorical data represent a predefined set of categories, they can't have values that go beyond these predefined categories.\n",
    "\n",
    "### Why could we Have these problems?\n",
    "\n",
    "* Data Entry Errors\n",
    "    - Free text\n",
    "    - Dropdowns\n",
    "* Parsing Errors\n",
    "\n",
    "### How do we treat these problems?\n",
    "\n",
    "* Dropping Data\n",
    "* Remapping Categories\n",
    "* Inferring Categories\n",
    "\n",
    "### An Example\n",
    "\n",
    "Here's a DataFrame named ``study_data`` containing a list of ``first names``, ``birth dates``, and ``blood types``. \n",
    "\n",
    "```python\n",
    "# Read study data and print it\n",
    "study_data = pd.read_csv('study.csv')\n",
    "study_data\n",
    "```\n",
    "\n",
    "<img src='pictures/study.jpg' width=450 allign= left />\n",
    "\n",
    "Additionally, a DataFrame named ``categories``, containing the correct possible categories for the blood type column has been created as well.\n",
    "\n",
    "```python\n",
    "# Correct possible blood types\n",
    "categories\n",
    "```\n",
    "\n",
    "<img src='pictures/blood.jpg' width=150/>\n",
    "\n",
    "Notice the inconsistency here? There's definitely no blood type named **Z+**. Luckily, the ``categories`` DataFrame will help us systematically spot all rows with these inconsistencies. \n",
    "\n",
    "It's always good practice to keep a log of all possible values of your categorical data, as it will make dealing with these types of inconsistencies way easier.\n",
    "\n",
    "### A note on Join\n",
    "\n",
    "Before moving on to dealing with these inconsistent values, let's have a brief reminder on joins. The two main types of joins we care about here are **anti joins** and **inner joins**.\n",
    "\n",
    "#### Anti Join\n",
    "\n",
    "**Anti joins**, take in *two* DataFrames A and B, and return data from one DataFrame that is not contained in another. \n",
    "\n",
    "<img src='pictures/antijoin.jpg' />\n",
    "\n",
    "In this example, we are performing a left anti join of A and B, and are returning the columns of DataFrames A and B for values only found in A of the common column between them being joined on.\n",
    "\n",
    "#### Inner Join\n",
    "\n",
    "Inner joins, return only the data that is contained in both DataFrames. \n",
    "\n",
    "<img src='pictures/innerjoin.jpg' />\n",
    "\n",
    "For example, an inner join of A and B, would return columns from both DataFrames for values only found in A and B, of the common column between them being joined on\n",
    "\n",
    "#### A left anti join on blood types\n",
    "\n",
    "* What is in ``study_data`` only\n",
    "    - Returns only rows containing Z+\n",
    "\n",
    "#### An inner join on blood types\n",
    "\n",
    "* What is in ``study_data`` and ``categories`` only\n",
    "    - Returns all rows except those containing Z+, B+ and AB-\n",
    "\n",
    "### Finding inconsistent categories\n",
    "\n",
    "Let's see how to do that in Python:\n",
    "\n",
    "```python\n",
    "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
    "print(inconsistent_categories)\n",
    "\n",
    "'''\n",
    "{'Z+'} # Output\n",
    "'''\n",
    "```\n",
    "\n",
    "We first get all inconsistent categories in the ``blood_type`` column of the ``study_data`` DataFrame. \n",
    "\n",
    "We do that by creating a *set* out of the ``blood_type`` column which stores its unique values, and use the ``difference()`` method which takes in as argument the ``blood_type`` column from the ``categories`` DataFrame. This returns all the categories in ``blood_type`` that are not in categories.\n",
    "\n",
    "```python\n",
    "# Get and print rows with inconsistent categories\n",
    "inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)\n",
    "study_data[inconsistent_rows]\n",
    "\n",
    "'''\n",
    "5 Jennifer 2019-12-17   Z+  # Output Row\n",
    "'''\n",
    "```\n",
    "\n",
    "We then find the inconsistent rows by finding all the rows of the ``blood_type`` columns that are equal to inconsistent categories by using the ``isin()`` method, this returns a series of boolean values that are ``True`` for inconsistent rows and ``False`` for consistent ones. \n",
    "\n",
    "We then subset the ``study_data`` DataFrame based on these boolean values, and voila we have our inconsistent data.\n",
    "\n",
    "### Dropping Inconsistent Categories\n",
    "\n",
    "To drop inconsistent rows and keep ones that are only consistent. We just use the tilde symbol while subsetting which returns everything except inconsistent rows.\n",
    "\n",
    "```python\n",
    "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
    "inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)\n",
    "inconsistent_data = study_data[inconsistent_rows]\n",
    "\n",
    "# Drop inconsistent categories and get consistent data only\n",
    "consistent_data = study_data[~inconsistent_rows]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Finding Consistency\n",
    "\n",
    "In this exercise and throughout this chapter, you'll be working with the ``airlines`` DataFrame which contains survey responses on the San Francisco Airport from airline customers.\n",
    "\n",
    "The DataFrame contains flight metadata such as the *airline*, the *destination*, *waiting times* as well as answers to key questions regarding *cleanliness*, *safety*, and *satisfaction*. Another DataFrame named ``categories`` was created, containing all correct possible values for the survey columns.\n",
    "\n",
    "In this exercise, you will use both of these DataFrames to find survey answers with inconsistent values, and drop them, effectively performing an outer and inner join on both these DataFrames as seen in the video exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cleanliness           safety          satisfaction\n",
      "0           Clean          Neutral        Very satisfied\n",
      "1         Average        Very safe               Neutral\n",
      "2  Somewhat clean    Somewhat_safe    Somewhat_satisfied\n",
      "3  Somewhat dirty      Very_unsafe  Somewhat_unsatisfied\n",
      "4           Dirty  Somewhat_unsafe      Very_unsatisfied\n",
      "Cleanliness:  ['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty'] \n",
      "\n",
      "Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
      "\n",
      "Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
      " 'Very unsatisfied'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "# Airlines data set\n",
    "airlines = pd.read_csv('datasets/airlines_final.csv')\n",
    "# Categories dataset\n",
    "categories = pd.read_csv('datasets/categories.csv')\n",
    "\n",
    "\n",
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, id, day, airline, destination, dest_region, dest_size, boarding_area, dept_time, wait_min, cleanliness, safety, satisfaction]\n",
      "Index: []\n",
      "      Unnamed: 0    id        day        airline        destination  \\\n",
      "0              0  1351    Tuesday    UNITED INTL             KANSAI   \n",
      "1              1   373     Friday         ALASKA  SAN JOSE DEL CABO   \n",
      "2              2  2820   Thursday          DELTA        LOS ANGELES   \n",
      "3              3  1157    Tuesday      SOUTHWEST        LOS ANGELES   \n",
      "4              4  2992  Wednesday       AMERICAN              MIAMI   \n",
      "...          ...   ...        ...            ...                ...   \n",
      "2472        2804  1475    Tuesday         ALASKA       NEW YORK-JFK   \n",
      "2473        2805  2222   Thursday      SOUTHWEST            PHOENIX   \n",
      "2474        2806  2684     Friday         UNITED            ORLANDO   \n",
      "2475        2807  2549    Tuesday        JETBLUE         LONG BEACH   \n",
      "2476        2808  2162   Saturday  CHINA EASTERN            QINGDAO   \n",
      "\n",
      "        dest_region dest_size boarding_area   dept_time  wait_min  \\\n",
      "0              Asia       Hub  Gates 91-102  2018-12-31     115.0   \n",
      "1     Canada/Mexico     Small   Gates 50-59  2018-12-31     135.0   \n",
      "2           West US       Hub   Gates 40-48  2018-12-31      70.0   \n",
      "3           West US       Hub   Gates 20-39  2018-12-31     190.0   \n",
      "4           East US       Hub   Gates 50-59  2018-12-31     559.0   \n",
      "...             ...       ...           ...         ...       ...   \n",
      "2472        East US       Hub   Gates 50-59  2018-12-31     280.0   \n",
      "2473        West US       Hub   Gates 20-39  2018-12-31     165.0   \n",
      "2474        East US       Hub   Gates 70-90  2018-12-31      92.0   \n",
      "2475        West US     Small    Gates 1-12  2018-12-31      95.0   \n",
      "2476           Asia     Large    Gates 1-12  2018-12-31     220.0   \n",
      "\n",
      "         cleanliness         safety        satisfaction  \n",
      "0              Clean        Neutral      Very satisfied  \n",
      "1              Clean      Very safe      Very satisfied  \n",
      "2            Average  Somewhat safe             Neutral  \n",
      "3              Clean      Very safe  Somewhat satsified  \n",
      "4     Somewhat clean      Very safe  Somewhat satsified  \n",
      "...              ...            ...                 ...  \n",
      "2472  Somewhat clean        Neutral  Somewhat satsified  \n",
      "2473           Clean      Very safe      Very satisfied  \n",
      "2474           Clean      Very safe      Very satisfied  \n",
      "2475           Clean  Somewhat safe      Very satisfied  \n",
      "2476           Clean      Very safe  Somewhat satsified  \n",
      "\n",
      "[2477 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson II\n",
    "\n",
    "## Categorical Variables\n",
    "\n",
    "We can have other types of problems that could affect categorical variables.\n",
    "\n",
    "### What type of errors could we have?\n",
    "\n",
    "-  **Value inconsistency**\n",
    "\n",
    "* *Inconsistent fields:* ``'married'``, ``'Married'``, ``'UNMARRIED'``, ``'not_married'`` ...\n",
    "* *_Trailing white spaces:* _``'married'``, ``'married '``...\n",
    "\n",
    "- **Collapsing too many categories to few**\n",
    "\n",
    "* *Creating new groups:* ``0-20K``, ``20-40K`` categories... from continuous household income data\n",
    "* *Mapping groups to new ones:* Mapping household income categories to 2 ``'rich'``, ``'poor'``\n",
    "\n",
    "- **Making sure data is of type**\n",
    "\n",
    "* ``Category`` (Seen in Chapter I)\n",
    "\n",
    "### Value consistency\n",
    "\n",
    "Let's start with making sure our categorical data is consistent. A common categorical data problem is having values that slightly differ because of **capitalization**.\n",
    "\n",
    "*  ``'married'``, ``'Married'``, ``'UNMARRIED'``, ``'not_married'``\n",
    "\n",
    "for example, let's assume we're working with a demographics dataset, and we have a marriage status column with inconsistent capitalization. \n",
    "\n",
    "```python\n",
    "# Get marriage status column\n",
    "marriage_status = demographics['marriage_status']\n",
    "marriage_status.value_counts()\n",
    "```\n",
    "Here's what counting the number of married people in the ``marriage_status`` *Series* would look like. Note that the ``.value_counts()`` methods works on *Series* only.\n",
    "\n",
    "<img src='pictures/valueconsistency.jpg' />\n",
    "\n",
    "For a DataFrame, we can groupby the column and use the ``.count()`` method.\n",
    "\n",
    "```python\n",
    "# Get value counts on DataFrame\n",
    "marriage_status.groupby('marriage_status').count()\n",
    "```\n",
    "\n",
    "To deal with this, we can either **capitalize** or **lowercase** the ``marriage_status`` column. This can be done with the ``str.upper()`` or ``.lower()`` functions respectively.\n",
    "\n",
    "```python\n",
    "# Capitalize\n",
    "marriage_status['marriage_status'] = marriage_status['marriage_status'].str.upper()\n",
    "\n",
    "# Lowercase\n",
    "marriage_status['marriage_status'] = marriage_status['marriage_status'].str.lower()\n",
    "```\n",
    "\n",
    "Another common problem with categorical values are *leading* or *trailing* spaces. \n",
    "\n",
    "* _``'married'``, ``'married '``...\n",
    "\n",
    "For example, imagine the same demographics DataFrame containing values with leading spaces. \n",
    "\n",
    "```python\n",
    "# Get marriage status column\n",
    "marriage_status = demographics['marriage_status']\n",
    "marriage_status.value_counts()\n",
    "```\n",
    "\n",
    "\n",
    "Here's what the counts of married vs unmarried people would look like. Note that there is a married category with a trailing space on the right, which makes it hard to spot on the output, as opposed to unmarried.\n",
    "\n",
    "<img src='pictures/trailing.jpg' />\n",
    "\n",
    "To remove leading spaces, we can use the ``str.strip()`` method which when given no input, strips all leading and trailing white spaces.\n",
    "\n",
    "```python\n",
    "# Strip all spaces\n",
    "demographics = demographics['marriage_status'].str.strip()\n",
    "```\n",
    "\n",
    "### Collapsing data into categories\n",
    "\n",
    "Sometimes, we may want to create categories out of our data, such as creating household income groups from income data. To create categories out of data, \n",
    "\n",
    "let's use the example of creating an ``income_group`` column in the demographics DataFrame. We can do this in 2 ways. \n",
    "\n",
    "```python\n",
    "# Using qcut()\n",
    "import pandas as pd\n",
    "group_names = ['0-200K', '200K-500K', '500K+']\n",
    "demographics['income_group'] = pd.qcut(demographics['household_income'], q=3,\n",
    "                                        labels=group_names)\n",
    "                                        \n",
    "```\n",
    "\n",
    "The first method utilizes the ``qcut()`` function from ``pandas``, which automatically divides our data based on its distribution into the number of categories we set in the ``q`` argument, we created the category names in the ``group_names`` list and fed it to the ``labels`` argument, returning the following. \n",
    "\n",
    "```python\n",
    "# Print income_group column\n",
    "demographics[['income_group', 'household_income']]\n",
    "```\n",
    "\n",
    "<img src='pictures/collapsing.jpg' />\n",
    "\n",
    "Notice that the first row actually misrepresents the actual income of the income group, as we didn't instruct qcut where our ranges actually lie.\n",
    "\n",
    "```python\n",
    "# Using cut() - create category ranges and names\n",
    "ranges = [0, 200000, 500000, np.inf]\n",
    "group_names = ['0-200K', '200K-500K', '500K+']\n",
    "\n",
    "# Crate income group column\n",
    "demographics['income_group'] = pd.cut(demographics['household_income'], bins=ranges, labels=group_names)\n",
    "```\n",
    "\n",
    "We can do this with the ``cut()`` function instead, which lets us define category cutoff ranges with the ``bins`` argument. It takes in a *list* of cutoff points for each category, with the final one being infinity represented with ``np.inf()``. \n",
    "\n",
    "```python\n",
    "demographics[['income_group', 'household_income']]\n",
    "```\n",
    "\n",
    "<img src='pictures/cut.jpg' />\n",
    "\n",
    "From the output, we can see this is much more correct.\n",
    "\n",
    "Sometimes, we may want to *reduce* the amount of categories we have in our data. Let's move on to mapping categories to fewer ones.\n",
    "\n",
    "* ``operating_system`` column is : ``'Microsoft'``, ``'MacOS'``, ``'IOS'``, ``'Android'``, ``'Linux'``\n",
    "* ``operating_system`` column **should** become: ``'DesktopOS'``, ``'MobileOS'``\n",
    "\n",
    "We can do this using the ``replace()`` method. It takes in a *dictionary* that maps each existing category to the category name you desire. \n",
    "\n",
    "```python\n",
    "# Create mapping dictionary and replace\n",
    "mapping = {'Microsoft':'DesktopOS', 'MacOS':'DesktopOS', 'IOS':'MobileOS', 'Android':'MobileOS', 'Linux':'DesktopOS'}\n",
    "devices['operating_system'] = devices['operating_system'].replace(mapping)\n",
    "```\n",
    "\n",
    "In this case, this is the mapping dictionary. A quick print of the unique values of operating system shows the mapping has been complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Inconsistent Categories\n",
    "\n",
    "In this exercise, you'll be revisiting the ``airlines`` DataFrame from the previous lesson.\n",
    "\n",
    "As a reminder, the DataFrame contains flight metadata such as the *airline*, the *destination*, *waiting times* as well as *answers* to key questions regarding *cleanliness*, *safety*, and *satisfaction* on the San Francisco Airport.\n",
    "\n",
    "In this exercise, you will examine *two* categorical columns from this DataFrame, ``dest_region`` and ``dest_size`` respectively, assess how to address them and make sure that they are cleaned and ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
      " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
      " 'Australia/New Zealand' 'middle east']\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']\n"
     ]
    }
   ],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
      " 'europe' 'central/south america' 'australia/new zealand']\n",
      "['Hub' 'Small' 'Medium' 'Large']\n"
     ]
    }
   ],
   "source": [
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remapping categories\n",
    "\n",
    "To better understand survey respondents from ``airlines``, you want to find out if there is a relationship between certain responses and the day of the week and wait time at the gate.\n",
    "\n",
    "The ``airlines`` DataFrame contains the ``day`` and ``wait_min`` columns, which are categorical and numerical respectively. The ``day`` column contains the exact day a flight took place, and ``wait_min`` contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new categorical variables:\n",
    "\n",
    "* ``wait_type``: ``'short'`` for 0-60 min, ``'medium'`` for 60-180 and ``long`` for 180+\n",
    "* ``day_week``: ``'weekday'`` if day is in the weekday, ``'weekend'`` if day is in the weekend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson III\n",
    "\n",
    "## Cleaning text Data\n",
    "\n",
    "In this lesson we'll talk about text data and regular expressions.\n",
    "\n",
    "### What is text data?\n",
    "\n",
    "text data is one of the most common types of data types.\n",
    "\n",
    "| **Type of data** | **Example values** |\n",
    "|--------------|----------------|\n",
    "| Names | ``Alex``, ``Sara`` ... |\n",
    "| Phone Numbers | +96171679912 ... |\n",
    "| Emails | 'adel@datacamp.com' |\n",
    "| Passwords | ----- |\n",
    "\n",
    "**Common text data problems:**\n",
    "\n",
    "- *Data inconsistency:*\n",
    "    * ``+96171679912`` or ``0096171679912``\n",
    "- *Fixed length violations:*\n",
    "    * Passwords needs to be at least 8 characters\n",
    "- *Typos:*\n",
    "    * ``+961.71.679912``\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's take a look at the following example. Here's a DataFrame named phones containing the full name and phone numbers of individuals. Both are string columns. Notice the ``phone number`` column.\n",
    "\n",
    "```python\n",
    "phones = pd.read_csv('phones.csv')\n",
    "print(phones)\n",
    "```\n",
    "\n",
    "<img src='pictures/phones.jpg' />\n",
    "\n",
    "We can see that there are phone number values, that begin with 00 or +. We also see that there is one entry where the phone number is 4 digits, which is non-existent. Furthermore, we can see that there are dashes across the phone number column. If we wanted to feed these phone numbers into an automated call system, or create a report discussing the distribution of users by area code, we couldn't really do so without uniform phone numbers.\n",
    "\n",
    "#### Fixing the phone number column\n",
    "\n",
    "Let's first begin by replacing the plus sign with 00, to do this, we use the dot str dot replace method which takes in two values, the string being replaced, which is in this case the plus sign and the string to replace it with which is in this case 00. \n",
    "\n",
    "```python\n",
    "# Replace \"+\" with \"00\"\n",
    "phones['Phone number'] = phones['Phone number'].str.replace('+', '00')\n",
    "```\n",
    "\n",
    "We can see that the column has been updated.\n",
    "\n",
    "```python\n",
    "phones\n",
    "```\n",
    "\n",
    "<img src='pictures/phones1.jpg' />\n",
    "\n",
    "We use the same exact technique to remove the dashes, by replacing the dash symbol with an empty string.\n",
    "\n",
    "```python\n",
    "# Replace \"-\" with nothing\n",
    "phones['Phone number'] = phones['Phone number'].str.replace(\"-\", \"\")\n",
    "```\n",
    "\n",
    "Now finally we're going to replace all phone numbers *below* 10 digits to ``NaN``. We can do this by chaining the Phone number column with the ``.str.len()`` method, which returns the *string* length of each row in the column. \n",
    "\n",
    "We can then use the ``.loc()`` method, to index rows where digits is below 10, and replace the value of Phone number with *numpy's* ``nan`` object, which is here imported as ``np``.\n",
    "\n",
    "```python\n",
    "# Replace phone numbers with lower than 10 digits to NaN\n",
    "digits = phones['Phone number'].str.len()\n",
    "phones.loc[digits < 10, \"Phone number\"] = np.nan\n",
    "```\n",
    "\n",
    "We can also write ``assert`` statements to test whether the phone number column has a specific length,and whether it contains the symbols we removed. \n",
    "\n",
    "```python\n",
    "# Find length of each row in Phone number column\n",
    "sanity_check = phone['Phone number'].str.len()\n",
    "```\n",
    "\n",
    "The first assert statement tests that the minimum length of the strings in the phone number column, found through ``str.len()``, is *bigger than or equal* to ``10``. \n",
    "\n",
    "```python\n",
    "# Assert minmum phone number length is 10\n",
    "assert sanity_check.min() <= 10\n",
    "```\n",
    "\n",
    "In the second assert statement, we use the ``str.contains()`` method to test whether the phone number column contains a specific pattern. It returns a series of booleans for that are ``True`` for matches and ``False`` for non-matches. \n",
    "We set the pattern **plus bar pipe minus**, the *bar pipe* here is basically an **or** statement, so we're trying to find matches for either symbols. We chain it with the any method which returns ``True`` if any element in the output of our ``.str.contains()`` is ``True``, and test whether the it returns ``False``.\n",
    "\n",
    "```python\n",
    "# Assert all numbers do not have \"+\" or \"-\"\n",
    "assert phone['Phone number'].str.contains(\"+|-\").any() == False\n",
    "```\n",
    "\n",
    "#### Regular Expressions in action\n",
    "\n",
    "But what about more complicated examples? How can we clean a phone number column that looks like this for example? \n",
    "\n",
    "<img src='pictures/re.jpg' />\n",
    "\n",
    "Where phone numbers can contain a range of symbols from *plus signs, dashes, parenthesis* and maybe more. This is where *regular expressions* come in. *Regular expressions* give us the ability to search for any pattern in text data, like only digits for example. They are like control + find in your browser, but way more dynamic and robust.\n",
    "\n",
    "Let's a look at this example:\n",
    "\n",
    "```python\n",
    "# Replace letters with nothing\n",
    "phones['Phone number'] = phones['Phone number'].str.replace(r'\\D+', '')\n",
    "```\n",
    "\n",
    "Here we are attempting to only extract digits from the phone number column. To do this, we use the  ``.str.replace()`` method with the pattern we want to replace with an empty string. Notice the pattern fed into the method. This is essentially us telling pandas to replace anything that is not a digit with nothing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Removing titles and taking names\n",
    "\n",
    "While collecting survey respondent metadata in the ``airlines`` DataFrame, the full name of respondents was saved in the ``full_name`` column. However upon closer inspection, you found that a lot of the different names are prefixed by honorifics such as ``\"Dr.\"``, ``\"Mr.\"``, ``\"Ms.\"`` and ``\"Miss\"``.\n",
    "\n",
    "Your ultimate objective is to create two new columns named ``first_name`` and ``last_name``, containing the first and last names of respondents respectively. Before doing so however, you need to remove honorifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE WILL ONLY WORK ON DATACAMP WORKSPACE\n",
    "\n",
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\",\"\")\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping it descriptive\n",
    "\n",
    "To further understand travelers' experiences in the San Francisco Airport, the quality assurance department sent out a qualitative questionnaire to all travelers who gave the airport the worst score on all possible categories. The objective behind this questionnaire is to identify common patterns in what travelers are saying about the airport.\n",
    "\n",
    "Their response is stored in the ``survey_response`` column. Upon a closer look, you realized a few of the answers gave the shortest possible character amount without much substance. In this exercise, you will isolate the responses with a character count higher than **40** , and make sure your new DataFrame contains responses with **40** characters or more using an ``assert`` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE WILL ONLY WORK ON DATACAMP WORKSPACE\n",
    "\n",
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c59e6e5379e7fd956ce48a476e9664867cfb6229c9530fa6fd87fb84f21040f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
