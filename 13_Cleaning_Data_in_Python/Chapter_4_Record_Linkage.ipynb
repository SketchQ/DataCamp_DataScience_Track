{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson I\n",
    "\n",
    "## Comparing Strings\n",
    "\n",
    "We'll discover the world of record linkage. But before we get deep dive into record linkage, let's sharpen our understanding of string similarity and minimum edit distance.\n",
    "\n",
    "### Minimum edit distance\n",
    "\n",
    "*Minimum edit distance* is a systematic way to identify how close 2 strings are. \n",
    "\n",
    "For example, let's take a look at the following two words: **intention**, and **execution**. The minimum edit distance between them is the least possible amount of steps, that could get us from the word intention to execution, with the available operations being:\n",
    "\n",
    "* Insertion\n",
    "* Deletion\n",
    "* Substitution\n",
    "* Transposition\n",
    "\n",
    "To get from **intention** to **execution** ;\n",
    "\n",
    "* Deleting \"I\" from intention\n",
    "* Adding \"C\" between \"E\" and \"N\"\n",
    "* Substitute the first \"N\" with \"E\", \n",
    "* \"T\" with \"X\" and\n",
    "* \"N\" with \"U\"\n",
    "\n",
    "***Minimum edit distance being 5!***\n",
    "\n",
    "The lower the edit distance, the closer two words are. For example, the two different typos of **reading** have a minimum edit distance of 1 between **reeding** and **reading**.\n",
    "\n",
    "#### Minimum edit distance Algorithms\n",
    "\n",
    "There's a variety of algorithms based on edit distance that differ on which operations they use, how much weight attributed to each operation, which type of strings they're suited for and more, with a variety of packages to get each similarity.\n",
    "\n",
    "| Algorithm | Operations|\n",
    "|-----------|-----------|\n",
    "| Damerau-Levenshtein | insertion, substitution, deletion, transposition |\n",
    "| **Levenshtein** | **insertion, substitution, deletion** |\n",
    "| Hamming | substitution only |\n",
    "| Jaro distance | transposition only |\n",
    "| ... | ... |\n",
    "\n",
    "**Possible packages :**\n",
    "* ``nltk``\n",
    "* ``fuzzywuzzy``\n",
    "* ``textdistance``\n",
    "\n",
    "For this lesson, we'll be comparing strings using *Levenshtein* distance since it's the most general form of string matching by using the **fuzzywuzzy** package.\n",
    "\n",
    "### Simple String Comparison\n",
    "\n",
    "``Fuzzywuzzy`` is a simple to use package to perform string comparison. \n",
    "We first ``import fuzz from fuzzywuzzy``, which allow us to compare between single strings. Here we use *fuzz*'s ``WRatio()`` function to compute the similarity between reading and its typo, inputting each string as an argument. \n",
    "For any comparison function using ``fuzzywuzzy``, our output is a score from **0** to **100** with 0 being not similar at all, 100 being an exact match. \n",
    "\n",
    "Do not confuse this with the minimum edit distance score earlier, where a lower minimum edit distance means a closer match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us compare between two strings\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Compare reeding vs reading\n",
    "fuzz.WRatio('Reeding', 'Reading')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial strings and different orderings\n",
    "\n",
    "The ``WRatio()`` function is highly robust against partial string comparison with different orderings. \n",
    "For example here we compare the strings *Houston Rockets and Rockets*, and still receive a high similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Partial string comparison\n",
    "fuzz.WRatio('Houston Rockets', 'Rockets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be said for the strings *Houston Rockets vs Los Angeles Lakers and Lakers vs Rockets*, where the team names are only partial and they are differently ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Partial string comparison with different order\n",
    "fuzz.WRatio('Houston Rockets vs Los Angeles Lakers', 'Lakers vs Rockets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparision with arrays\n",
    "\n",
    "We can also compare a string with an *array* of strings by using the ``extract()`` function from the ``process module from fuzzy wuzzy``. \n",
    "``extract()`` takes in a *string*, *an array of strings*, and *the number of possible matches to return ranked from highest to lowest*. \n",
    "It *returns* a **list of tuples with 3 elements**; \n",
    "* the first one being the matching string being returned, \n",
    "* the second one being its similarity score, \n",
    "* and the third one being its index in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rockets vs Lakers', 86, 0), ('Lakers vs Rockets', 86, 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import process\n",
    "from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "\n",
    "# Define string and array of possible matches\n",
    "string = \"Houston Rockets vs Los Angeles Lakers\"\n",
    "choices = pd.Series([\"Rockets vs Lakers\", \"Lakers vs Rockets\",\n",
    "                     \"Houston vs Los Angeles\", \"Heat vs Bulls\"])\n",
    "\n",
    "process.extract(string, choices, limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapsing Categories with string similarity\n",
    "\n",
    "In chapter 2, we learned that collapsing data into categories is an essential aspect of working with categorical and text data, and we saw how to manually replace categories in a column of a DataFrame. But what if we had so many inconsistent categories that a manual replacement is simply not feasible? We can easily do that with string similarity!\n",
    "\n",
    "* Use ``.replace()`` to collapse ``\"eur\"`` into ``\"Europe\"``\n",
    "\n",
    "* What if there are too many variations?\n",
    "    - ``\"EU\", \"eur\", \"Europ\", \"Europa\", \"Erope\", \"Evropa\"`` ...\n",
    "\n",
    "Say we have DataFrame named ``survey`` containing answers from respondents from the state of New York and California asking them how likely are you to move on a scale of 0 to 5. \n",
    "\n",
    "```python\n",
    "print(survey['state'].unique())\n",
    "```\n",
    "\n",
    "<img src='pictures/survey.jpg' width=250 />\n",
    "\n",
    "The state field was free text and contains hundreds of typos. Remapping them manually would take a huge amount of time. Instead, we'll use string similarity. \n",
    "\n",
    "We also have a ``category`` DataFrame containing the correct categories for each state(``'California'`` ``'New York'``). Let's collapse the incorrect categories with string matching!\n",
    "\n",
    "#### Collapsing all of the state\n",
    "\n",
    "We first create a *for loop* iterating over each correctly typed state in the ``categories`` DataFrame. \n",
    "\n",
    "For each state, we find its matches in the ``state`` column of the ``survey`` DataFrame, returning all possible matches by setting the ``limit`` argument of extract to the length of the ``survey`` DataFrame.\n",
    "\n",
    "Then we iterate over each potential match, isolating the ones only with a similarity score higher or equal than 80 with an if statement. \n",
    "\n",
    "Then for each of those returned strings, we replace it with the correct state using the .``loc`` method.\n",
    "\n",
    "```python\n",
    "# For Each correct category\n",
    "for state in categories['state']:\n",
    "    # Find potential matches in states with typoes\n",
    "    matches = process.extract(state, survey['state'], limit = survey.shape[0])\n",
    "    # For each potential match match\n",
    "    for potential_match in matches:\n",
    "        # If high similarity score\n",
    "        if potential_match[1] >= 80:\n",
    "            # Replace typo with correct category\n",
    "            survey.loc[survey['state'] == potential_match[0], 'state'] = state\n",
    "```\n",
    "\n",
    "### Record Linkage\n",
    "\n",
    "<img src='pictures/recordlinkage.jpg' />\n",
    "\n",
    "Record linkage attempts to join data sources that have similarly fuzzy duplicate values, so that we end up with a final DataFrame with no duplicates by using string similarity. We'll cover record linkage in more detail in the next couple of lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "### The cutoff point\n",
    "\n",
    "In this exercise, and throughout this chapter, you'll be working with the ``restaurants`` DataFrame which has data on various restaurants. Your ultimate goal is to create a restaurant recommendation engine, but you need to first clean your data.\n",
    "\n",
    "This version of restaurants has been collected from many sources, where the ``cuisine_type`` column is riddled with typos, and should contain only *italian, american and asian* cuisine types. There are so many unique categories that remapping them manually isn't scalable, and it's best to use string similarity instead.\n",
    "\n",
    "Before doing so, you want to establish the cutoff point for the similarity score using the *fuzzywuzzy*'s ``process.extract()`` function by finding the similarity score of the most distant typo of each category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c59e6e5379e7fd956ce48a476e9664867cfb6229c9530fa6fd87fb84f21040f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
