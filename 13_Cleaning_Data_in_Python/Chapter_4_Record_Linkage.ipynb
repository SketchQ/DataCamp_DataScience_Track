{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson I\n",
    "\n",
    "## Comparing Strings\n",
    "\n",
    "We'll discover the world of record linkage. But before we get deep dive into record linkage, let's sharpen our understanding of string similarity and minimum edit distance.\n",
    "\n",
    "### Minimum edit distance\n",
    "\n",
    "*Minimum edit distance* is a systematic way to identify how close 2 strings are. \n",
    "\n",
    "For example, let's take a look at the following two words: **intention**, and **execution**. The minimum edit distance between them is the least possible amount of steps, that could get us from the word intention to execution, with the available operations being:\n",
    "\n",
    "* Insertion\n",
    "* Deletion\n",
    "* Substitution\n",
    "* Transposition\n",
    "\n",
    "To get from **intention** to **execution** ;\n",
    "\n",
    "* Deleting \"I\" from intention\n",
    "* Adding \"C\" between \"E\" and \"N\"\n",
    "* Substitute the first \"N\" with \"E\", \n",
    "* \"T\" with \"X\" and\n",
    "* \"N\" with \"U\"\n",
    "\n",
    "***Minimum edit distance being 5!***\n",
    "\n",
    "The lower the edit distance, the closer two words are. For example, the two different typos of **reading** have a minimum edit distance of 1 between **reeding** and **reading**.\n",
    "\n",
    "#### Minimum edit distance Algorithms\n",
    "\n",
    "There's a variety of algorithms based on edit distance that differ on which operations they use, how much weight attributed to each operation, which type of strings they're suited for and more, with a variety of packages to get each similarity.\n",
    "\n",
    "| Algorithm | Operations|\n",
    "|-----------|-----------|\n",
    "| Damerau-Levenshtein | insertion, substitution, deletion, transposition |\n",
    "| **Levenshtein** | **insertion, substitution, deletion** |\n",
    "| Hamming | substitution only |\n",
    "| Jaro distance | transposition only |\n",
    "| ... | ... |\n",
    "\n",
    "**Possible packages :**\n",
    "* ``nltk``\n",
    "* ``fuzzywuzzy``\n",
    "* ``textdistance``\n",
    "\n",
    "For this lesson, we'll be comparing strings using *Levenshtein* distance since it's the most general form of string matching by using the **fuzzywuzzy** package.\n",
    "\n",
    "### Simple String Comparison\n",
    "\n",
    "``Fuzzywuzzy`` is a simple to use package to perform string comparison. \n",
    "We first ``import fuzz from fuzzywuzzy``, which allow us to compare between single strings. Here we use *fuzz*'s ``WRatio()`` function to compute the similarity between reading and its typo, inputting each string as an argument. \n",
    "For any comparison function using ``fuzzywuzzy``, our output is a score from **0** to **100** with 0 being not similar at all, 100 being an exact match. \n",
    "\n",
    "Do not confuse this with the minimum edit distance score earlier, where a lower minimum edit distance means a closer match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us compare between two strings\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Compare reeding vs reading\n",
    "fuzz.WRatio('Reeding', 'Reading')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial strings and different orderings\n",
    "\n",
    "The ``WRatio()`` function is highly robust against partial string comparison with different orderings. \n",
    "For example here we compare the strings *Houston Rockets and Rockets*, and still receive a high similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Partial string comparison\n",
    "fuzz.WRatio('Houston Rockets', 'Rockets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be said for the strings *Houston Rockets vs Los Angeles Lakers and Lakers vs Rockets*, where the team names are only partial and they are differently ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Partial string comparison with different order\n",
    "fuzz.WRatio('Houston Rockets vs Los Angeles Lakers', 'Lakers vs Rockets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparision with arrays\n",
    "\n",
    "We can also compare a string with an *array* of strings by using the ``extract()`` function from the ``process module from fuzzy wuzzy``. \n",
    "``extract()`` takes in a *string*, *an array of strings*, and *the number of possible matches to return ranked from highest to lowest*. \n",
    "It *returns* a **list of tuples with 3 elements**; \n",
    "* the first one being the matching string being returned, \n",
    "* the second one being its similarity score, \n",
    "* and the third one being its index in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rockets vs Lakers', 86, 0), ('Lakers vs Rockets', 86, 1)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import process\n",
    "from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "\n",
    "# Define string and array of possible matches\n",
    "string = \"Houston Rockets vs Los Angeles Lakers\"\n",
    "choices = pd.Series([\"Rockets vs Lakers\", \"Lakers vs Rockets\",\n",
    "                     \"Houston vs Los Angeles\", \"Heat vs Bulls\"])\n",
    "\n",
    "process.extract(string, choices, limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapsing Categories with string similarity\n",
    "\n",
    "In chapter 2, we learned that collapsing data into categories is an essential aspect of working with categorical and text data, and we saw how to manually replace categories in a column of a DataFrame. But what if we had so many inconsistent categories that a manual replacement is simply not feasible? We can easily do that with string similarity!\n",
    "\n",
    "* Use ``.replace()`` to collapse ``\"eur\"`` into ``\"Europe\"``\n",
    "\n",
    "* What if there are too many variations?\n",
    "    - ``\"EU\", \"eur\", \"Europ\", \"Europa\", \"Erope\", \"Evropa\"`` ...\n",
    "\n",
    "Say we have DataFrame named ``survey`` containing answers from respondents from the state of New York and California asking them how likely are you to move on a scale of 0 to 5. \n",
    "\n",
    "```python\n",
    "print(survey['state'].unique())\n",
    "```\n",
    "\n",
    "<img src='pictures/survey.jpg' width=250 />\n",
    "\n",
    "The state field was free text and contains hundreds of typos. Remapping them manually would take a huge amount of time. Instead, we'll use string similarity. \n",
    "\n",
    "We also have a ``category`` DataFrame containing the correct categories for each state(``'California'`` ``'New York'``). Let's collapse the incorrect categories with string matching!\n",
    "\n",
    "#### Collapsing all of the state\n",
    "\n",
    "We first create a *for loop* iterating over each correctly typed state in the ``categories`` DataFrame. \n",
    "\n",
    "For each state, we find its matches in the ``state`` column of the ``survey`` DataFrame, returning all possible matches by setting the ``limit`` argument of extract to the length of the ``survey`` DataFrame.\n",
    "\n",
    "Then we iterate over each potential match, isolating the ones only with a similarity score higher or equal than 80 with an if statement. \n",
    "\n",
    "Then for each of those returned strings, we replace it with the correct state using the .``loc`` method.\n",
    "\n",
    "```python\n",
    "# For Each correct category\n",
    "for state in categories['state']:\n",
    "    # Find potential matches in states with typoes\n",
    "    matches = process.extract(state, survey['state'], limit = survey.shape[0])\n",
    "    # For each potential match match\n",
    "    for potential_match in matches:\n",
    "        # If high similarity score\n",
    "        if potential_match[1] >= 80:\n",
    "            # Replace typo with correct category\n",
    "            survey.loc[survey['state'] == potential_match[0], 'state'] = state\n",
    "```\n",
    "\n",
    "### Record Linkage\n",
    "\n",
    "<img src='pictures/recordlinkage.jpg' />\n",
    "\n",
    "Record linkage attempts to join data sources that have similarly fuzzy duplicate values, so that we end up with a final DataFrame with no duplicates by using string similarity. We'll cover record linkage in more detail in the next couple of lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "### The cutoff point\n",
    "\n",
    "In this exercise, and throughout this chapter, you'll be working with the ``restaurants`` DataFrame which has data on various restaurants. Your ultimate goal is to create a restaurant recommendation engine, but you need to first clean your data.\n",
    "\n",
    "This version of restaurants has been collected from many sources, where the ``cuisine_type`` column is riddled with typos, and should contain only *italian, american and asian* cuisine types. There are so many unique categories that remapping them manually isn't scalable, and it's best to use string similarity instead.\n",
    "\n",
    "Before doing so, you want to establish the cutoff point for the similarity score using the *fuzzywuzzy*'s ``process.extract()`` function by finding the similarity score of the most distant typo of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asian', 100), ('indonesian', 72), ('italian', 67), ('russian', 67), ('american', 62), ('californian', 54), ('japanese', 54), ('mexican/tex-mex', 54), ('american ( new )', 54), ('mexican', 50), ('pizza', 40), ('cajun/creole', 36), ('diners', 36), ('middle eastern', 36), ('vietnamese', 36), ('pacific new wave', 36), ('fast food', 36), ('continental', 36), ('seafood', 33), ('chicken', 33), ('chinese', 33), ('hamburgers', 27), ('steakhouses', 25), ('southern/soul', 22), ('delis', 20), ('hot dogs', 18), ('coffee shops', 18), ('noodle shops', 18), ('health food', 18), ('eclectic', 18), ('coffeebar', 18), ('french ( new )', 18), ('desserts', 18)]\n",
      "[('american', 100), ('american ( new )', 90), ('mexican', 80), ('mexican/tex-mex', 68), ('asian', 62), ('californian', 53), ('italian', 53), ('russian', 53), ('middle eastern', 45), ('pacific new wave', 45), ('hamburgers', 44), ('indonesian', 44), ('chicken', 40), ('japanese', 38), ('eclectic', 38), ('delis', 36), ('pizza', 36), ('southern/soul', 34), ('french ( new )', 34), ('vietnamese', 33), ('cajun/creole', 30), ('diners', 29), ('seafood', 27), ('chinese', 27), ('desserts', 25), ('coffeebar', 24), ('steakhouses', 21), ('health food', 21), ('continental', 21), ('fast food', 12), ('coffee shops', 11), ('noodle shops', 11), ('hot dogs', 0)]\n",
      "[('italian', 100), ('asian', 67), ('californian', 56), ('american', 53), ('continental', 51), ('indonesian', 47), ('russian', 43), ('mexican', 43), ('japanese', 40), ('mexican/tex-mex', 39), ('american ( new )', 39), ('pacific new wave', 39), ('middle eastern', 38), ('vietnamese', 35), ('delis', 33), ('pizza', 33), ('diners', 31), ('chicken', 29), ('chinese', 29), ('health food', 27), ('eclectic', 27), ('steakhouses', 26), ('southern/soul', 26), ('cajun/creole', 21), ('seafood', 14), ('hot dogs', 13), ('noodle shops', 13), ('french ( new )', 13), ('desserts', 13), ('hamburgers', 12), ('fast food', 12), ('coffeebar', 12), ('coffee shops', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Restaurant dataset\n",
    "restaurant = pd.read_csv('datasets/restaurants_L2_dirty.csv')\n",
    "\n",
    "# Import process from fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurant['type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian',unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american',unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian',unique_types, limit = len(unique_types)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remapping Categories II\n",
    "\n",
    "In the last exercise, you determined that the distance cutoff point for remapping typos of *'american', 'asian', and 'italian'* cuisine types stored in the ``cuisine_type`` column should be **80**.\n",
    "\n",
    "In this exercise, you're going to put it all together by finding matches with similarity scores equal to or higher than 80 by using **fuzywuzzy.process**'s ``extract()`` function, for each correct cuisine type, and replacing these matches with it. Remember, when comparing a string with an array of strings using ``process.extract()``, the output is a list of tuples where each is formatted like:\n",
    "\n",
    "```python\n",
    "(closest match, similarity score, index of match)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american' 'californian' 'japanese' 'cajun/creole' 'hot dogs' 'diners'\n",
      " 'delis' 'hamburgers' 'seafood' 'italian' 'coffee shops' 'russian'\n",
      " 'steakhouses' 'mexican/tex-mex' 'noodle shops' 'mexican' 'middle eastern'\n",
      " 'asian' 'vietnamese' 'health food' 'american ( new )' 'pacific new wave'\n",
      " 'indonesian' 'eclectic' 'chicken' 'fast food' 'southern/soul' 'coffeebar'\n",
      " 'continental' 'french ( new )' 'desserts' 'chinese' 'pizza']\n"
     ]
    }
   ],
   "source": [
    "# Categories array\n",
    "import numpy as np\n",
    "categories_np = np.array(['american', 'asian', 'italian'])\n",
    "# Categories df\n",
    "categories = pd.DataFrame(categories_np, columns=['type'])\n",
    "\n",
    "# Inspect the unique values of the cuisine_type column\n",
    "print(restaurant['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('italian', 100, 14), ('italian', 100, 21), ('italian', 100, 47), ('italian', 100, 57), ('italian', 100, 73)]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurant['type'], limit = len(restaurant['type']))\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the list of matches to italian\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "        restaurant.loc[restaurant['type'] == match[0], 'type'] = 'italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american' 'californian' 'japanese' 'cajun/creole' 'hot dogs' 'diners'\n",
      " 'delis' 'hamburgers' 'seafood' 'italian' 'coffee shops' 'russian'\n",
      " 'steakhouses' 'mexican/tex-mex' 'noodle shops' 'mexican' 'middle eastern'\n",
      " 'asian' 'vietnamese' 'health food' 'american ( new )' 'pacific new wave'\n",
      " 'indonesian' 'eclectic' 'chicken' 'fast food' 'southern/soul' 'coffeebar'\n",
      " 'continental' 'french ( new )' 'desserts' 'chinese' 'pizza']\n"
     ]
    }
   ],
   "source": [
    "# Iterate through categories\n",
    "for cuisine in categories:\n",
    "    # Create a list of matches, comparing the cuisine with the type column\n",
    "    matches = process.extract(cuisine, restaurant['type'], limit = len(restaurant['type']))\n",
    "    \n",
    "    # Iterate through the list of matches\n",
    "    for match in matches:\n",
    "        # Check whether the similarity score is greater than or equal to 80\n",
    "        if match[1] >= 80:\n",
    "            # Select all rows where the type is spelled this way, and set them to the correct cuisine\n",
    "            restaurant.loc[restaurant['type'] == match[0], 'type'] = cuisine\n",
    "            \n",
    "# Inspect the final result\n",
    "print(restaurant['type'].unique())     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson II\n",
    "\n",
    "## Generating Pairs\n",
    "\n",
    "At the end of the last video exercise, we saw how record linkage attempts to join data sources with fuzzy duplicate values. \n",
    "\n",
    "<img src='pictures/recordlinkage.jpg' />\n",
    "\n",
    "For example here are two DataFrames containing NBA games and their schedules. They've both been scraped from different sites and we would want to merge them together and have one DataFrame containing all unique games.\n",
    "\n",
    "We see that there are duplicates values in both DataFrames with different naming marked here in red, and non duplicate values, marked here in green. Since there are games happening at the same time, no common unique identifier between the DataFrames, and the events are differently named, a regular **join** or **merge** will not work. This is where **record linkage** comes in.\n",
    "\n",
    "### Record Linkage\n",
    "\n",
    "Record linkage is the act of linking data from different sources regarding the same entity. \n",
    "\n",
    "* Generally, we clean *two or more* DataFrames, \n",
    "* *Generate* pairs of potentially matching records, \n",
    "* *Score(Compare)* these pairs according to string similarity and other similarity metrics, \n",
    "* And *link* them. \n",
    "\n",
    "All of these steps can be achieved with the ``recordlinkage`` package, let's find how!\n",
    "\n",
    "#### Our DataFrames\n",
    "\n",
    "```census_A```\n",
    "\n",
    "<img src='pictures/censusA.jpg' />\n",
    "\n",
    "```census_B```\t\n",
    "\n",
    "<img src='pictures/censusB.jpg' />\n",
    "\n",
    "Here we have two DataFrames, ``census_A``, and ``census_B``, containing data on individuals throughout the states. \n",
    "We want to merge them while avoiding duplication using *record linkage*, since they are collected manually and are prone to typos, there are no consistent IDs between them.\n",
    "\n",
    "#### Generating Pairs\n",
    "\n",
    "We first want to generate pairs between both DataFrames. Ideally, we want to generate all possible pairs between our DataFrames.\n",
    "\n",
    "What if we had big DataFrames and ended up having to generate millions if not billions of pairs? It wouldn't prove scalable and could seriously hamper development time.\n",
    "\n",
    "##### Blocking\n",
    "\n",
    "This is where we apply what we call **blocking**, which creates pairs based on a matching column, which is in this case, the ``state`` column, reducing the number of possible pairs.\n",
    "\n",
    "To do this, we first start off by importing ``recordlinkage``. \n",
    "\n",
    "```python\n",
    "# Import recordlinkage\n",
    "import recordlinkage\n",
    "```\n",
    "\n",
    "We then use the ``recordlinkage.Index()`` function, to create an *indexing* object. This essentially is an object we can use to generate pairs from our DataFrames. \n",
    "\n",
    "```python\n",
    "# Create indexing object\n",
    "indexer = recordlinkage.Index()\n",
    "```\n",
    "\n",
    "To generate pairs blocked on state, we use the ``block()`` method, inputting the ``state`` column as input. \n",
    "\n",
    "```python\n",
    "# Generate pairs blocked on state\n",
    "indexer.block('state')\n",
    "```\n",
    "\n",
    "Once the *indexer* object has been initialized, we generate our pairs using the ``.index()`` method, which takes in the *two dataframes*.\n",
    "\n",
    "```python\n",
    "pairs = indexer.index(census_A, census_B)\n",
    "```\n",
    "\n",
    "The resulting object, is a ``pandas`` multi index object containing pairs of row indices from both DataFrames, which is a fancy way to say it is an *array* containing possible pairs of indices that makes it much easier to subset DataFrames on.\n",
    "\n",
    "<img src='pictures/pairs.jpg' />\n",
    "\n",
    "#### Comparing the DataFrames\n",
    "\n",
    "Since we've already generated our pairs, it's time to find potential matches. We first start by creating a *comparison* object using the ``recordlinkage.compare()`` function. This is similar to the indexing object we created while generating pairs, but this one is responsible for assigning different comparison procedures for pairs. \n",
    "\n",
    "```python\n",
    "# Genarate the pairs\n",
    "pairs = indexer.index(census_A, census_B)\n",
    "# Create a Compare object\n",
    "compare_cl = recordlinkage.Compare()\n",
    "```\n",
    "\n",
    "Let's say there are columns for which we want exact matches between the pairs. To do that, we use the ``exact`` method. It takes in the column name in question for each DataFrame, which is in this case ``date_of_birth`` and ``state``, and a ``label`` argument which lets us set the column name in the resulting DataFrame. \n",
    "\n",
    "```python\n",
    "# Find exact matches for pairs of date_of_birth and state\n",
    "compare_cl.exact('date_of_birth', 'date_of_birth', label='date_of_birth_state')\n",
    "compare_cl.exact('state', 'state', label='state')\n",
    "```\n",
    "\n",
    "Now in order to compute string similarities between pairs of rows for columns that have fuzzy values, we use the ``.string()`` method, which also takes in the column names in question, the similarity *cutoff* point in the ``threshold`` argument, which takes in a value between ``0`` and ``1``, which we here set to ``0.85``. \n",
    "\n",
    "```python\n",
    "# Find similar matches for pairs of surname and address_1 using string similarity\n",
    "compare_cl.string('surname', 'surname', threshold=0.85, label='surname')\n",
    "compare_cl.string('address_1', 'address_1', threshold=0.85, label='address_1')\n",
    "```\n",
    "\n",
    "Finally to compute the matches, we use the ``compute()`` function, which takes in the possible pairs, and the two DataFrames in question. \n",
    "Note that you need to **always** have the **same order of DataFrames** when inserting them as arguments when generating pairs, comparing between columns, and computing comparisons.\n",
    "\n",
    "```python\n",
    "# Find Matches\n",
    "potential_matches = compare_cl.compute(pairs, census_A, census_B)\n",
    "```\n",
    "\n",
    "The output is a *multi-index* DataFrame, where the first index is the row index from the first DataFrame, or *census A*, and the second index is a list of all row indices in *census B*. The columns are the columns being compared, with values being **1** for a match, and **0** for not a match.\n",
    "\n",
    "<img src='pictures/potential_matches.jpg' />\n",
    "\n",
    "To find potential matches, we just filter for rows where the sum of row values is higher than a certain threshold. Which in this case higher or equal to 2. \n",
    "\n",
    "```python\n",
    "potential_matches[potential_matches.sum(axis=1) >= 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Pairs of restaurants\n",
    "\n",
    "In the last lesson, you cleaned the restaurants dataset to make it ready for building a restaurants recommendation engine. You have a new DataFrame named ``restaurants_new`` with new restaurants to train your model on, that's been scraped from a new data source.\n",
    "\n",
    "You've already cleaned the ``cuisine_type`` and ``city`` columns using the techniques learned throughout the course. However you saw duplicates with typos in restaurants names that require record linkage instead of joins with restaurants.\n",
    "\n",
    "In this exercise, you will perform the first step in record linkage and generate possible pairs of rows between ``restaurants`` and ``restaurants_new``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "restaurants = pd.read_csv('datasets/restaurants_L2_dirty.csv')\n",
    "restaurants_new = pd.read_csv('datasets/restaurants_L2.csv')\n",
    "\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "import recordlinkage\n",
    "\n",
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('type', 'type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurants,restaurants_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar Restaurants\n",
    "\n",
    "In the last exercise, you generated pairs between ``restaurants`` and ``restaurants_new`` in an effort to cleanly merge both DataFrames using record linkage.\n",
    "\n",
    "When performing record linkage, there are different types of matching you can perform between different columns of your DataFrames, including exact matches, string similarities, and more.\n",
    "\n",
    "Now that your pairs have been generated and stored in pairs, you will find exact matches in the ``city`` and ``cuisine_type`` columns between each pair, and similar strings for each pair in the ``rest_name`` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        city  type  name\n",
      "0  0       0     1   0.0\n",
      "   1       0     1   0.0\n",
      "   2       0     1   0.0\n",
      "   3       0     1   0.0\n",
      "   4       0     1   0.0\n",
      "...      ...   ...   ...\n",
      "55 221     1     1   0.0\n",
      "   230     1     1   0.0\n",
      "   233     1     1   0.0\n",
      "   238     1     1   0.0\n",
      "   241     1     1   0.0\n",
      "\n",
      "[3631 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, types\n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('type', 'type', label='type')\n",
    "\n",
    "# Find similar matches of name\n",
    "comp_cl.string('name', 'name', threshold=0.8 ,label='name')\n",
    "\n",
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city  type  name\n",
       "1  3      1     1   1.0\n",
       "7  13     1     1   1.0\n",
       "12 17     1     1   1.0\n",
       "20 20     1     1   1.0\n",
       "27 21     1     1   1.0\n",
       "28 1      1     1   1.0\n",
       "40 0      1     1   1.0\n",
       "43 8      1     1   1.0\n",
       "50 9      1     1   1.0\n",
       "53 4      1     1   1.0\n",
       "67 14     1     1   1.0\n",
       "74 2      1     1   1.0\n",
       "21 11     1     1   1.0\n",
       "47 19     1     1   1.0\n",
       "57 16     1     1   1.0\n",
       "73 6      1     1   1.0\n",
       "75 10     1     1   1.0\n",
       "26 12     1     1   1.0\n",
       "65 5      1     1   1.0\n",
       "71 18     1     1   1.0\n",
       "79 7      1     1   1.0\n",
       "55 15     1     1   1.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_matches[potential_matches.sum(axis=1) >= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson III\n",
    "\n",
    "## Linking DataFrames\n",
    "\n",
    "At this point, you've generated your pairs, compared them, and scored them. Now it's time to link your data!\n",
    "\n",
    "Remember our census DataFrames from the video of the previous lesson? We've already generated pairs between them, compared four of their columns, two for exact matches and two for string similarity alongside a 0.85 threshold, and found potential matches.\n",
    "\n",
    "Now it's time to link both census DataFrames. Let's look closely at our potential matches. It is a multi-index DataFrame, where we have two index columns, record id 1, and record id 2.\n",
    "\n",
    "The first index column, stores indices from ``census A``. The second index column, stores all possible indices from ``census_B``, for each row index of ``census_A``. The columns of our potential matches are the columns we chose to link both DataFrames on, where the value is *1* for a match, and *0* otherwise.\n",
    "\n",
    "<img src='pictures/potential_matches1.jpg' />\n",
    "\n",
    "The first step in linking DataFrames, is to isolate the potentially matching pairs to the ones we're pretty sure of. We saw how to do this in the previous lesson, by subsetting the rows where the row sum is above a certain number of columns, in this case 3. \n",
    "\n",
    "```python\n",
    "matches = potential_matches[potential_matches.sum(axis=1) >= 3]\n",
    "print(matches)\n",
    "```\n",
    "\n",
    "The output is row indices between ``census A`` and ``census B`` that are most likely duplicates. Our next step is to extract the one of the index columns, and subsetting its associated DataFrame to filter for duplicates.\n",
    "\n",
    "<img src='pictures/potential_matches2.jpg' />\n",
    "\n",
    "Here we choose the second index column, which represents row indices of ``census B``. We want to extract those indices, and *subset* ``census_B`` on them to remove duplicates with ``census_A`` before appending them together.\n",
    "\n",
    "We can access a DataFrame's index using the ``index`` attribute. Since this is a multi index DataFrame, it returns a multi index object containing pairs of row indices from ``census_A`` and ``census_B`` respectively. \n",
    "\n",
    "```python\n",
    "matches.index\n",
    "```\n",
    "\n",
    "We want to extract all ``census_B`` indices, so we chain it with the ``get_level_values()`` method, which takes in which column index we want to extract its values. We can either input the index column's name, or its order, which is in this case 1.\n",
    "\n",
    "```python\n",
    "# Get indices from census_B only\n",
    "duplicate_rows = matches.index.get_level_values(1)\n",
    "```\n",
    "\n",
    "To find the duplicates in ``census B``, we simply subset on all indices of ``census_B``, with the ones found through record linkage. You can choose to examine them further for similarity with their duplicates in ``census_A``, but if you're sure of your analysis, you can go ahead and find the non duplicates by repeating the exact same line of code, except by adding a *tilde* at the beginning of your subset. \n",
    "\n",
    "```python\n",
    "# Finding duplicates in census_B\n",
    "census_B_duplicates = census_B[census_B.index.isin(duplicate_rows)]\n",
    "\n",
    "# Finding new rows in census_B\n",
    "census_B_new = census_B[~census_B.index.isin(duplicate_rows)]\n",
    "```\n",
    "Now that you have your non duplicates, all you need is a simple ``append`` using the DataFrame append method of census A, and you have your linked Data!\n",
    "\n",
    "```python\n",
    "# Link the DataFrames\n",
    "full_census = census_A.append(census_B_new)\n",
    "```\n",
    "\n",
    "To recap, what we did was build on top of our previous work in generating pairs, comparing across columns and finding potential matches. \n",
    "We then isolated all possible matches, where there are matches across 3 columns or more, ensuring we tightened our search for duplicates across both DataFrames before we link them. \n",
    "Extracted the row indices of census_B where there are duplicates. Found rows of census_B where they are not duplicated with census_A by using the tilde symbol. \n",
    "And linked both DataFrames for full census results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "### Linking them together\n",
    "\n",
    "In the last lesson, you've finished the bulk of the work on your effort to link ``restaurants`` and ``restaurants_new``. You've generated the different pairs of potentially matching rows, searched for exact matches between the ``type`` and ``city`` columns, but compared for similar strings in the ``name`` column. You stored the DataFrame containing the scores in ``potential_matches``.\n",
    "\n",
    "Now it's finally time to link both DataFrames. You will do so by first extracting all row indices of ``restaurants_new`` that are matching across the columns mentioned above from ``potential_matches``. Then you will subset ``restaurants_new`` on these indices, then append the non-duplicate values to restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0                name                      addr           city  \\\n",
      "0             0              kokomo         6333 w. third st.             la   \n",
      "1             1              feenix   8358 sunset blvd. west       hollywood   \n",
      "2             2             parkway      510 s. arroyo pkwy .       pasadena   \n",
      "3             3                r-23          923 e. third st.    los angeles   \n",
      "4             4               gumbo         6333 w. third st.             la   \n",
      "..          ...                 ...                       ...            ...   \n",
      "331         331   vivande porta via        2125 fillmore st.   san francisco   \n",
      "332         332  vivande ristorante     670 golden gate ave.   san francisco   \n",
      "333         333        world wrapps        2257 chestnut st.   san francisco   \n",
      "334         334             wu kong            101 spear st.   san francisco   \n",
      "335         335           yank sing          427 battery st.   san francisco   \n",
      "\n",
      "          phone          type  \n",
      "0    2139330773      american  \n",
      "1    2138486677      american  \n",
      "2    8187951001   californian  \n",
      "3    2136877178      japanese  \n",
      "4    2139330358  cajun/creole  \n",
      "..          ...           ...  \n",
      "331  4153464430       italian  \n",
      "332  4156739245       italian  \n",
      "333  4155639727      american  \n",
      "334  4159579300         asian  \n",
      "335  4155414949         asian  \n",
      "\n",
      "[396 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devil\\AppData\\Local\\Temp\\ipykernel_20984\\3469608181.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  full_restaurants = restaurants.append(non_dup)\n"
     ]
    }
   ],
   "source": [
    "# Isolate the potential matches with row sum >= 3\n",
    "matches = potential_matches[potential_matches.sum(axis=1) >= 3]\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = restaurants_new.loc[~restaurants_new.index.isin(matching_indices)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.append(non_dup)\n",
    "print(full_restaurants)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c59e6e5379e7fd956ce48a476e9664867cfb6229c9530fa6fd87fb84f21040f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
